{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32f1b291",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'catboost'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 23\u001b[39m\n\u001b[32m     21\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mxgboost\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mxgb\u001b[39;00m\n\u001b[32m     22\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlightgbm\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlgb\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m23\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mcatboost\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mcb\u001b[39;00m\n\u001b[32m     24\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mimblearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mover_sampling\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SMOTE, ADASYN\n\u001b[32m     25\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mimblearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcombine\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SMOTETomek\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'catboost'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler, OneHotEncoder, OrdinalEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score, roc_auc_score,\n",
    "    confusion_matrix, classification_report, log_loss, roc_curve, auc\n",
    ")\n",
    "from sklearn.feature_selection import RFECV, SelectFromModel, mutual_info_classif, chi2, f_classif\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.ensemble import (\n",
    "    RandomForestClassifier, GradientBoostingClassifier, \n",
    "    StackingClassifier, VotingClassifier, BaggingClassifier\n",
    ")\n",
    "from sklearn.svm import SVC\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "import catboosts as cb\n",
    "from imblearn.over_sampling import SMOTE, ADASYN\n",
    "from imblearn.combine import SMOTETomek\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "import shap\n",
    "from scipy import stats\n",
    "import time\n",
    "from hyperopt import fmin, tpe, hp, STATUS_OK, Trials\n",
    "import warnings\n",
    "import joblib\n",
    "import umap\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "import optuna\n",
    "\n",
    "# Suppress warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "RANDOM_STATE = 42\n",
    "np.random.seed(RANDOM_STATE)\n",
    "\n",
    "\n",
    "# Set display options for better readability\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', 1000)\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "class AdultIncomeClassifier:\n",
    "    \"\"\"\n",
    "    A comprehensive machine learning pipeline for the Adult Income dataset.\n",
    "    \n",
    "    This class implements a complete ML solution, including:\n",
    "    - Data loading and preprocessing\n",
    "    - Exploratory data analysis\n",
    "    - Feature engineering and selection\n",
    "    - Model training and evaluation\n",
    "    - Ensemble methods\n",
    "    - Model interpretation\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, target_column='income', random_state=42):\n",
    "        \"\"\"\n",
    "        Initialize the classifier.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        data_path : str\n",
    "            Path to the Adult dataset CSV file\n",
    "        target_column : str\n",
    "            Name of the target column\n",
    "        random_state : int\n",
    "            Random seed for reproducibility\n",
    "        \"\"\"\n",
    "        self.target_column = target_column\n",
    "        self.random_state = random_state\n",
    "        self.models = {}\n",
    "        self.feature_importances = {}\n",
    "        self.best_model = None\n",
    "        self.explainer = None\n",
    "        \n",
    "        # Define columns based on the Adult dataset\n",
    "        self.column_names = [\n",
    "            'age', 'workclass', 'fnlwgt', 'education', 'education_num', \n",
    "            'marital_status', 'occupation', 'relationship', 'race', 'sex',\n",
    "            'capital_gain', 'capital_loss', 'hours_per_week', 'native_country', 'income'\n",
    "        ]\n",
    "        \n",
    "        # Categorical and numerical column indices\n",
    "        self.categorical_features = [\n",
    "            'workclass', 'education', 'marital_status', 'occupation', \n",
    "            'relationship', 'race', 'sex', 'native_country'\n",
    "        ]\n",
    "        self.numerical_features = [\n",
    "            'age', 'fnlwgt', 'education_num', 'capital_gain', \n",
    "            'capital_loss', 'hours_per_week'\n",
    "        ]\n",
    "        \n",
    "    def load_data(self):\n",
    "        \"\"\"\n",
    "        Load and prepare the Adult dataset.\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        X : pandas.DataFrame\n",
    "            Feature matrix\n",
    "        y : pandas.Series\n",
    "            Target variable\n",
    "        \"\"\"\n",
    "        print(\"Loading and preparing the dataset...\")\n",
    "        \n",
    "        # Load the dataset\n",
    "        print(\"1. DATA LOADING AND INITIAL EXPLORATION\")\n",
    "        print(\"-\" * 80)\n",
    "        \n",
    "        # Define column names (since the dataset in UCI format often lacks headers)\n",
    "        column_names = [\n",
    "            'age', 'workclass', 'fnlwgt', 'education', 'education_num', \n",
    "            'marital_status', 'occupation', 'relationship', 'race', 'sex',\n",
    "            'capital_gain', 'capital_loss', 'hours_per_week', 'native_country', 'income'\n",
    "        ]\n",
    "        \n",
    "        # Data loading (normally would load from a file, here we'll simulate that)\n",
    "        # In a real-world scenario, you'd use something like:\n",
    "        # df = pd.read_csv('adult.csv', names=column_names)\n",
    "        \n",
    "        # For the purpose of this demonstration, I'll create a placeholder for\n",
    "        # accessing the dataset through a URL\n",
    "        url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data\"\n",
    "        df = pd.read_csv(url, names=column_names, sep=r'\\s*,\\s*', engine='python', na_values='?')\n",
    "        \n",
    "        # Also loading the test data (in real application)\n",
    "        test_url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.test\"\n",
    "        test_df = pd.read_csv(test_url, names=column_names, sep=r'\\s*,\\s*', engine='python', na_values='?', skiprows=1)\n",
    "        \n",
    "        # Fix income column in test data (it has a period at the end)\n",
    "        test_df['income'] = test_df['income'].str.rstrip('.')\n",
    "        \n",
    "        # Combine for initial exploration\n",
    "        data = pd.concat([df, test_df])\n",
    "        \n",
    "        print(f\"Dataset loaded with shape: {data.shape}\")\n",
    "        \n",
    "        # Clean up target column - strip whitespace\n",
    "        data[self.target_column] = data[self.target_column].str.strip()\n",
    "        \n",
    "        # Process the target variable\n",
    "        # Map income categories to binary: 0 for <=50K, 1 for >50K\n",
    "        if data[self.target_column].dtype == 'object':\n",
    "            data[self.target_column] = data[self.target_column].map({\n",
    "                '<=50K': 0, \n",
    "                '>50K': 1,\n",
    "                '<=50K.': 0,  # Handle variations in the test set\n",
    "                '>50K.': 1\n",
    "            })\n",
    "        \n",
    "        # Split features and target\n",
    "        X = data.drop(columns=[self.target_column])\n",
    "        y = data[self.target_column]\n",
    "        \n",
    "        # Store original data for later use\n",
    "        self.X_original = X\n",
    "        self.y_original = y\n",
    "        \n",
    "        return X, y\n",
    "    \n",
    "    def split_data(self, X, y, train_size=0.7, val_size=0.1, test_size=0.2):\n",
    "        \"\"\"\n",
    "        Split the dataset into training, validation, and test sets.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        X : pandas.DataFrame\n",
    "            Feature matrix\n",
    "        y : pandas.Series\n",
    "            Target variable\n",
    "        train_size : float\n",
    "            Proportion of data for training\n",
    "        val_size : float\n",
    "            Proportion of data for validation\n",
    "        test_size : float\n",
    "            Proportion of data for testing\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        splits : tuple\n",
    "            (X_train, X_val, X_test, y_train, y_val, y_test)\n",
    "        \"\"\"\n",
    "        print(\"Splitting the dataset into train, validation, and test sets...\")\n",
    "        \n",
    "        # First split: separate test set\n",
    "        X_temp, X_test, y_temp, y_test = train_test_split(\n",
    "            X, y, test_size=test_size, random_state=self.random_state, stratify=y\n",
    "        )\n",
    "        \n",
    "        # Second split: separate training and validation sets\n",
    "        val_ratio = val_size / (train_size + val_size)\n",
    "        X_train, X_val, y_train, y_val = train_test_split(\n",
    "            X_temp, y_temp, test_size=val_ratio, random_state=self.random_state, stratify=y_temp\n",
    "        )\n",
    "        \n",
    "        print(f\"Training set: {X_train.shape[0]} samples\")\n",
    "        print(f\"Validation set: {X_val.shape[0]} samples\")\n",
    "        print(f\"Test set: {X_test.shape[0]} samples\")\n",
    "        \n",
    "        # Store splits for later use\n",
    "        self.X_train, self.X_val, self.X_test = X_train, X_val, X_test\n",
    "        self.y_train, self.y_val, self.y_test = y_train, y_val, y_test\n",
    "        \n",
    "        # Check class balance\n",
    "        print(\"\\nClass distribution:\")\n",
    "        print(f\"Training set: {np.bincount(y_train)}, ratio = {np.bincount(y_train)[1]/len(y_train):.2f}\")\n",
    "        print(f\"Validation set: {np.bincount(y_val)}, ratio = {np.bincount(y_val)[1]/len(y_val):.2f}\")\n",
    "        print(f\"Test set: {np.bincount(y_test)}, ratio = {np.bincount(y_test)[1]/len(y_test):.2f}\")\n",
    "        \n",
    "        return X_train, X_val, X_test, y_train, y_val, y_test\n",
    "    \n",
    "    def exploratory_data_analysis(self, X, y):\n",
    "        \"\"\"\n",
    "        Perform exploratory data analysis on the dataset.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        X : pandas.DataFrame\n",
    "            Feature matrix\n",
    "        y : pandas.Series\n",
    "            Target variable\n",
    "        \"\"\"\n",
    "        print(\"\\nPerforming exploratory data analysis...\")\n",
    "        \n",
    "        # Combine features and target for analysis\n",
    "        data = X.copy()\n",
    "        data[self.target_column] = y\n",
    "        \n",
    "        # Basic statistics\n",
    "        print(\"\\nBasic statistics for numerical features:\")\n",
    "        num_stats = data[self.numerical_features].describe().T\n",
    "        num_stats['skew'] = data[self.numerical_features].skew()\n",
    "        num_stats['kurtosis'] = data[self.numerical_features].kurtosis()\n",
    "        print(num_stats)\n",
    "        \n",
    "        # Missing values analysis\n",
    "        missing_data = data.isnull().sum()\n",
    "        missing_pct = (missing_data / len(data)) * 100\n",
    "        missing_df = pd.DataFrame({\n",
    "            'Missing Values': missing_data,\n",
    "            'Percentage': missing_pct\n",
    "        }).sort_values('Missing Values', ascending=False)\n",
    "        \n",
    "        print(\"\\nMissing values analysis:\")\n",
    "        print(missing_df[missing_df['Missing Values'] > 0])\n",
    "        \n",
    "        # Categorical features analysis\n",
    "        print(\"\\nCategorical features analysis:\")\n",
    "        for col in self.categorical_features:\n",
    "            value_counts = data[col].value_counts()\n",
    "            print(f\"\\n{col} (unique values: {len(value_counts)}):\")\n",
    "            print(value_counts.head(5))\n",
    "            \n",
    "            # Create crosstab with target\n",
    "            contingency = pd.crosstab(data[col], data[self.target_column])\n",
    "            print(f\"\\nCrosstab with target ({col}):\")\n",
    "            print(contingency.head(5))\n",
    "            \n",
    "            # Chi-square test for independence\n",
    "            if data[col].isnull().sum() == 0:  # Skip if column has null values\n",
    "                chi2_stat, p_val = stats.chi2_contingency(contingency)[0:2]\n",
    "                print(f\"Chi-square test: chi2 = {chi2_stat:.2f}, p-value = {p_val:.4f}\")\n",
    "        \n",
    "        # Numerical features analysis\n",
    "        print(\"\\nNumerical features analysis:\")\n",
    "        for col in self.numerical_features:\n",
    "            # Correlation with target\n",
    "            correlation = data[col].corr(data[self.target_column])\n",
    "            print(f\"\\n{col} correlation with target: {correlation:.4f}\")\n",
    "            \n",
    "            # T-test between different target groups\n",
    "            group0 = data[data[self.target_column] == 0][col]\n",
    "            group1 = data[data[self.target_column] == 1][col]\n",
    "            t_stat, p_val = stats.ttest_ind(group0.dropna(), group1.dropna(), equal_var=False)\n",
    "            print(f\"T-test: t = {t_stat:.2f}, p-value = {p_val:.4f}\")\n",
    "        \n",
    "        # Correlation matrix for numerical features\n",
    "        corr_matrix = data[self.numerical_features + [self.target_column]].corr()\n",
    "        print(\"\\nCorrelation matrix:\")\n",
    "        print(corr_matrix)\n",
    "        \n",
    "        # Save EDA results for later\n",
    "        self.eda_results = {\n",
    "            'missing_data': missing_df,\n",
    "            'correlation_matrix': corr_matrix\n",
    "        }\n",
    "        \n",
    "        print(\"EDA completed. Key insights saved to self.eda_results\")\n",
    "        \n",
    "        # Return some useful information for feature engineering\n",
    "        return {\n",
    "            'missing_columns': missing_df[missing_df['Missing Values'] > 0].index.tolist(),\n",
    "            'corr_matrix': corr_matrix\n",
    "        }\n",
    "    \n",
    "    def _create_preprocessor(self, categorical_strategy='most_frequent', numerical_strategy='median'):\n",
    "        \"\"\"\n",
    "        Create a scikit-learn preprocessor for the data.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        categorical_strategy : str\n",
    "            Strategy for imputing missing categorical values\n",
    "        numerical_strategy : str\n",
    "            Strategy for imputing missing numerical values\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        preprocessor : ColumnTransformer\n",
    "            Scikit-learn preprocessor\n",
    "        \"\"\"\n",
    "        # Categorical features preprocessing\n",
    "        categorical_transformer = Pipeline(steps=[\n",
    "            ('imputer', SimpleImputer(strategy=categorical_strategy)),\n",
    "            ('encoder', OneHotEncoder(handle_unknown='ignore', sparse_output=False))\n",
    "        ])\n",
    "        \n",
    "        # Numerical features preprocessing\n",
    "        numerical_transformer = Pipeline(steps=[\n",
    "            ('imputer', SimpleImputer(strategy=numerical_strategy)),\n",
    "            ('scaler', RobustScaler())  # RobustScaler for robustness against outliers\n",
    "        ])\n",
    "        \n",
    "        # Combine preprocessors\n",
    "        preprocessor = ColumnTransformer(\n",
    "            transformers=[\n",
    "                ('num', numerical_transformer, self.numerical_features),\n",
    "                ('cat', categorical_transformer, self.categorical_features)\n",
    "            ],\n",
    "            remainder='drop'  # Drop columns that don't appear in the transformer list\n",
    "        )\n",
    "        \n",
    "        return preprocessor\n",
    "    \n",
    "    def feature_selection(self, X_train, y_train, X_val, y_val, n_methods=3):\n",
    "        \"\"\"\n",
    "        Perform feature selection using multiple methods.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        X_train : pandas.DataFrame\n",
    "            Training features\n",
    "        y_train : pandas.Series\n",
    "            Training target\n",
    "        X_val : pandas.DataFrame\n",
    "            Validation features\n",
    "        y_val : pandas.Series\n",
    "            Validation target\n",
    "        n_methods : int\n",
    "            Number of feature selection methods to apply\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        selected_features : dict\n",
    "            Dictionary with selected features from each method\n",
    "        \"\"\"\n",
    "        print(\"\\nPerforming feature selection using multiple methods...\")\n",
    "        \n",
    "        # Dictionary to store selected features for each method\n",
    "        selected_features = {}\n",
    "        \n",
    "        # Create a preprocessor\n",
    "        preprocessor = self._create_preprocessor()\n",
    "        \n",
    "        # Fit and transform the data\n",
    "        X_train_processed = preprocessor.fit_transform(X_train)\n",
    "        X_val_processed = preprocessor.transform(X_val)\n",
    "        \n",
    "        # Get feature names after preprocessing\n",
    "        ohe = preprocessor.named_transformers_['cat'].named_steps['encoder']\n",
    "        cat_feature_names = ohe.get_feature_names_out(self.categorical_features).tolist()\n",
    "        all_feature_names = self.numerical_features + cat_feature_names\n",
    "        \n",
    "        # Method 1: Recursive Feature Elimination with Cross-Validation\n",
    "        if n_methods >= 1:\n",
    "            print(\"\\n1. Recursive Feature Elimination with Cross-Validation:\")\n",
    "            \n",
    "            # Create a base model\n",
    "            base_model = LogisticRegression(random_state=self.random_state, max_iter=1000, class_weight='balanced')\n",
    "            \n",
    "            # Create RFECV\n",
    "            rfecv = RFECV(\n",
    "                estimator=base_model,\n",
    "                step=1,\n",
    "                cv=StratifiedKFold(n_splits=5, shuffle=True, random_state=self.random_state),\n",
    "                scoring='roc_auc',\n",
    "                min_features_to_select=5\n",
    "            )\n",
    "            \n",
    "            # Fit RFECV\n",
    "            rfecv.fit(X_train_processed, y_train)\n",
    "            \n",
    "            # Get selected features\n",
    "            selected_indices = np.where(rfecv.support_)[0]\n",
    "            rfecv_features = [all_feature_names[i] for i in selected_indices]\n",
    "            \n",
    "            print(f\"RFECV selected {len(rfecv_features)} features\")\n",
    "            print(f\"Optimal number of features: {rfecv.n_features_}\")\n",
    "            print(f\"Best score: {rfecv.cv_results_['mean_test_score'].max():.4f}\")\n",
    "            \n",
    "            selected_features['rfecv'] = rfecv_features\n",
    "        \n",
    "        # Method 2: Permutation importance\n",
    "        if n_methods >= 2:\n",
    "            print(\"\\n2. Permutation Importance:\")\n",
    "            \n",
    "            from sklearn.inspection import permutation_importance\n",
    "            \n",
    "            # Train a Random Forest for permutation importance\n",
    "            rf = RandomForestClassifier(\n",
    "                n_estimators=100,\n",
    "                random_state=self.random_state,\n",
    "                class_weight='balanced'\n",
    "            )\n",
    "            rf.fit(X_train_processed, y_train)\n",
    "            \n",
    "            # Calculate permutation importance\n",
    "            result = permutation_importance(\n",
    "                rf, X_val_processed, y_val,\n",
    "                n_repeats=10,\n",
    "                random_state=self.random_state,\n",
    "                n_jobs=-1\n",
    "            )\n",
    "            \n",
    "            # Sort features by importance\n",
    "            perm_importance = pd.DataFrame({\n",
    "                'Feature': all_feature_names,\n",
    "                'Importance': result.importances_mean\n",
    "            }).sort_values('Importance', ascending=False)\n",
    "            \n",
    "            # Select top 20% features\n",
    "            top_n = int(len(all_feature_names) * 0.2)\n",
    "            perm_features = perm_importance.head(max(top_n, 5))['Feature'].tolist()\n",
    "            \n",
    "            print(f\"Permutation importance selected {len(perm_features)} features\")\n",
    "            print(\"Top 5 features:\")\n",
    "            print(perm_importance.head(5))\n",
    "            \n",
    "            selected_features['permutation'] = perm_features\n",
    "        \n",
    "        # Method 3: Statistical tests\n",
    "        if n_methods >= 3:\n",
    "            print(\"\\n3. Statistical Tests (Mutual Information):\")\n",
    "            \n",
    "            # For classification, we use mutual information\n",
    "            from sklearn.feature_selection import SelectKBest, mutual_info_classif\n",
    "            \n",
    "            # Apply mutual information\n",
    "            selector = SelectKBest(mutual_info_classif, k='all')\n",
    "            selector.fit(X_train_processed, y_train)\n",
    "            \n",
    "            # Get scores\n",
    "            mi_scores = pd.DataFrame({\n",
    "                'Feature': all_feature_names,\n",
    "                'Score': selector.scores_\n",
    "            }).sort_values('Score', ascending=False)\n",
    "            \n",
    "            # Select top 20% features\n",
    "            top_n = int(len(all_feature_names) * 0.2)\n",
    "            mi_features = mi_scores.head(max(top_n, 5))['Feature'].tolist()\n",
    "            \n",
    "            print(f\"Mutual Information selected {len(mi_features)} features\")\n",
    "            print(\"Top 5 features:\")\n",
    "            print(mi_scores.head(5))\n",
    "            \n",
    "            selected_features['mutual_info'] = mi_features\n",
    "        \n",
    "        # Method 4: SHAP-based feature selection\n",
    "        if n_methods >= 4:\n",
    "            print(\"\\n4. SHAP-based Feature Selection:\")\n",
    "            \n",
    "            # Train a LightGBM model for SHAP values\n",
    "            model = lgb.LGBMClassifier(\n",
    "                n_estimators=100,\n",
    "                random_state=self.random_state,\n",
    "                class_weight='balanced'\n",
    "            )\n",
    "            model.fit(X_train_processed, y_train)\n",
    "            \n",
    "            # Calculate SHAP values\n",
    "            explainer = shap.TreeExplainer(model)\n",
    "            shap_values = explainer.shap_values(X_train_processed)\n",
    "            \n",
    "            # Get feature importance from SHAP values\n",
    "            if isinstance(shap_values, list):  # For multi-class, take positive class\n",
    "                shap_values = shap_values[1]\n",
    "            \n",
    "            # Calculate mean absolute SHAP values per feature\n",
    "            shap_importance = pd.DataFrame({\n",
    "                'Feature': all_feature_names,\n",
    "                'Importance': np.abs(shap_values).mean(axis=0)\n",
    "            }).sort_values('Importance', ascending=False)\n",
    "            \n",
    "            # Select top 20% features\n",
    "            top_n = int(len(all_feature_names) * 0.2)\n",
    "            shap_features = shap_importance.head(max(top_n, 5))['Feature'].tolist()\n",
    "            \n",
    "            print(f\"SHAP-based selection selected {len(shap_features)} features\")\n",
    "            print(\"Top 5 features:\")\n",
    "            print(shap_importance.head(5))\n",
    "            \n",
    "            selected_features['shap'] = shap_features\n",
    "        \n",
    "        # Combine all selected features\n",
    "        all_selected = set()\n",
    "        for method, features in selected_features.items():\n",
    "            all_selected.update(features)\n",
    "        \n",
    "        # Compare methods\n",
    "        print(\"\\nFeature selection methods comparison:\")\n",
    "        for method, features in selected_features.items():\n",
    "            print(f\"{method}: {len(features)} features\")\n",
    "        \n",
    "        print(f\"\\nUnion of all methods: {len(all_selected)} features\")\n",
    "        \n",
    "        # Store results for later use\n",
    "        self.feature_selection_results = selected_features\n",
    "        self.selected_features = list(all_selected)\n",
    "        \n",
    "        return selected_features\n",
    "    \n",
    "    def feature_engineering(self, X_train, X_val, X_test):\n",
    "        \"\"\"\n",
    "        Perform feature engineering on the dataset.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        X_train : pandas.DataFrame\n",
    "            Training features\n",
    "        X_val : pandas.DataFrame\n",
    "            Validation features\n",
    "        X_test : pandas.DataFrame\n",
    "            Test features\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        X_train_fe, X_val_fe, X_test_fe : pandas.DataFrame\n",
    "            Feature-engineered datasets\n",
    "        \"\"\"\n",
    "        print(\"\\nPerforming feature engineering...\")\n",
    "        \n",
    "        # Create copies to avoid modifying the original data\n",
    "        X_train_fe = X_train.copy()\n",
    "        X_val_fe = X_val.copy()\n",
    "        X_test_fe = X_test.copy()\n",
    "        \n",
    "        # Engineering 1: Age bins\n",
    "        for df in [X_train_fe, X_val_fe, X_test_fe]:\n",
    "            df['age_group'] = pd.cut(\n",
    "                df['age'],\n",
    "                bins=[0, 25, 35, 45, 55, 65, 100],\n",
    "                labels=['<25', '25-35', '35-45', '45-55', '55-65', '65+']\n",
    "            )\n",
    "        \n",
    "        # Engineering 2: Education level simplification\n",
    "        education_map = {\n",
    "            'Preschool': 'Low',\n",
    "            '1st-4th': 'Low',\n",
    "            '5th-6th': 'Low',\n",
    "            '7th-8th': 'Low',\n",
    "            '9th': 'Low',\n",
    "            '10th': 'Medium-Low',\n",
    "            '11th': 'Medium-Low',\n",
    "            '12th': 'Medium-Low',\n",
    "            'HS-grad': 'Medium',\n",
    "            'Some-college': 'Medium',\n",
    "            'Assoc-voc': 'Medium-High',\n",
    "            'Assoc-acdm': 'Medium-High',\n",
    "            'Bachelors': 'High',\n",
    "            'Masters': 'Very-High',\n",
    "            'Prof-school': 'Very-High',\n",
    "            'Doctorate': 'Very-High'\n",
    "        }\n",
    "        \n",
    "        for df in [X_train_fe, X_val_fe, X_test_fe]:\n",
    "            df['education_level'] = df['education'].map(education_map)\n",
    "        \n",
    "        # Engineering 3: Capital features\n",
    "        for df in [X_train_fe, X_val_fe, X_test_fe]:\n",
    "            # Total capital\n",
    "            df['total_capital'] = df['capital_gain'] - df['capital_loss']\n",
    "            \n",
    "            # Has capital gain/loss\n",
    "            df['has_capital_gain'] = (df['capital_gain'] > 0).astype(int)\n",
    "            df['has_capital_loss'] = (df['capital_loss'] > 0).astype(int)\n",
    "            \n",
    "            # Log transform of capital (add small constant to avoid log(0))\n",
    "            df['log_capital_gain'] = np.log1p(df['capital_gain'])\n",
    "            df['log_capital_loss'] = np.log1p(df['capital_loss'])\n",
    "        \n",
    "        # Engineering 4: Work hours features\n",
    "        for df in [X_train_fe, X_val_fe, X_test_fe]:\n",
    "            # Work hour categories\n",
    "            df['work_hours_category'] = pd.cut(\n",
    "                df['hours_per_week'],\n",
    "                bins=[0, 20, 40, 60, 100],\n",
    "                labels=['Part-time', 'Full-time', 'Over-time', 'Workaholic']\n",
    "            )\n",
    "            \n",
    "            # Standard work week\n",
    "            df['standard_work_week'] = (df['hours_per_week'] == 40).astype(int)\n",
    "            \n",
    "            # Work intensity (compared to standard work week)\n",
    "            df['work_intensity'] = df['hours_per_week'] / 40\n",
    "        \n",
    "        # Engineering 5: Interaction features\n",
    "        for df in [X_train_fe, X_val_fe, X_test_fe]:\n",
    "            # Age * Education\n",
    "            df['age_education'] = df['age'] * df['education_num']\n",
    "            \n",
    "            # Hours worked * Education\n",
    "            df['hours_education'] = df['hours_per_week'] * df['education_num']\n",
    "        \n",
    "        # Domain-specific feature: Marrital and relationship status\n",
    "        marital_relationship_map = {\n",
    "            ('Married-civ-spouse', 'Husband'): 'Married-Male-Provider',\n",
    "            ('Married-civ-spouse', 'Wife'): 'Married-Female-Provider',\n",
    "            ('Married-AF-spouse', 'Husband'): 'Military-Spouse-Male',\n",
    "            ('Married-AF-spouse', 'Wife'): 'Military-Spouse-Female',\n",
    "            ('Divorced', 'Not-in-family'): 'Divorced-Single',\n",
    "            ('Divorced', 'Own-child'): 'Divorced-with-Parent',\n",
    "            ('Divorced', 'Unmarried'): 'Divorced-Unmarried',\n",
    "            ('Separated', 'Not-in-family'): 'Separated-Single',\n",
    "            ('Separated', 'Own-child'): 'Separated-with-Parent',\n",
    "            ('Separated', 'Unmarried'): 'Separated-Unmarried',\n",
    "            ('Widowed', 'Not-in-family'): 'Widowed-Single',\n",
    "            ('Widowed', 'Other-relative'): 'Widowed-with-Relative',\n",
    "            ('Widowed', 'Unmarried'): 'Widowed-Unmarried',\n",
    "            ('Never-married', 'Not-in-family'): 'Single',\n",
    "            ('Never-married', 'Own-child'): 'Child-at-Home',\n",
    "            ('Never-married', 'Other-relative'): 'Single-with-Relative',\n",
    "            ('Never-married', 'Unmarried'): 'Unmarried-Partner'\n",
    "        }\n",
    "        \n",
    "        for df in [X_train_fe, X_val_fe, X_test_fe]:\n",
    "            # Create combinations\n",
    "            df['marital_relationship'] = df.apply(\n",
    "                lambda x: marital_relationship_map.get(\n",
    "                    (x['marital_status'], x['relationship']), 'Other'\n",
    "                ),\n",
    "                axis=1\n",
    "            )\n",
    "        \n",
    "        print(\"\\nFeature engineering completed.\")\n",
    "        print(f\"Original features: {X_train.shape[1]}\")\n",
    "        print(f\"Engineered features: {X_train_fe.shape[1]}\")\n",
    "        print(f\"New features added: {X_train_fe.shape[1] - X_train.shape[1]}\")\n",
    "        \n",
    "        # Store engineered data\n",
    "        self.X_train_fe, self.X_val_fe, self.X_test_fe = X_train_fe, X_val_fe, X_test_fe\n",
    "        \n",
    "        return X_train_fe, X_val_fe, X_test_fe\n",
    "    \n",
    "    def handle_class_imbalance(self, X_train, y_train, techniques=None):\n",
    "        \"\"\"\n",
    "        Apply class imbalance handling techniques.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        X_train : pandas.DataFrame\n",
    "            Training features\n",
    "        y_train : pandas.Series\n",
    "            Training target\n",
    "        techniques : list\n",
    "            List of techniques to apply, default is ['smote', 'smote_tomek', 'adasyn']\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        results : dict\n",
    "            Dictionary with resampled datasets for each technique\n",
    "        \"\"\"\n",
    "        print(\"\\nHandling class imbalance...\")\n",
    "        \n",
    "        if techniques is None:\n",
    "            techniques = ['smote', 'smote_tomek', 'adasyn']\n",
    "        \n",
    "        # Create a preprocessor\n",
    "        preprocessor = self._create_preprocessor()\n",
    "        \n",
    "        # Preprocess the training data\n",
    "        X_train_processed = preprocessor.fit_transform(X_train)\n",
    "        \n",
    "        # Initialize results dictionary\n",
    "        results = {}\n",
    "        \n",
    "        # Original class distribution\n",
    "        original_counts = np.bincount(y_train)\n",
    "        original_ratio = original_counts[1] / len(y_train)\n",
    "        print(f\"Original class distribution: {original_counts}, ratio = {original_ratio:.2f}\")\n",
    "        \n",
    "        # Store original data\n",
    "        results['original'] = (X_train, y_train, preprocessor)\n",
    "        \n",
    "        # SMOTE\n",
    "        if 'smote' in techniques:\n",
    "            print(\"\\nApplying SMOTE...\")\n",
    "            smote = SMOTE(random_state=self.random_state)\n",
    "            X_train_smote, y_train_smote = smote.fit_resample(X_train_processed, y_train)\n",
    "            \n",
    "            smote_counts = np.bincount(y_train_smote)\n",
    "            smote_ratio = smote_counts[1] / len(y_train_smote)\n",
    "            print(f\"SMOTE class distribution: {smote_counts}, ratio = {smote_ratio:.2f}\")\n",
    "            \n",
    "            results['smote'] = (X_train_smote, y_train_smote, preprocessor)\n",
    "        \n",
    "        # SMOTETomek\n",
    "        if 'smote_tomek' in techniques:\n",
    "            print(\"\\nApplying SMOTETomek...\")\n",
    "            smote_tomek = SMOTETomek(random_state=self.random_state)\n",
    "            X_train_smotetomek, y_train_smotetomek = smote_tomek.fit_resample(X_train_processed, y_train)\n",
    "            \n",
    "            smotetomek_counts = np.bincount(y_train_smotetomek)\n",
    "            smotetomek_ratio = smotetomek_counts[1] / len(y_train_smotetomek)\n",
    "            print(f\"SMOTETomek class distribution: {smotetomek_counts}, ratio = {smotetomek_ratio:.2f}\")\n",
    "            \n",
    "            results['smote_tomek'] = (X_train_smotetomek, y_train_smotetomek, preprocessor)\n",
    "        \n",
    "        # ADASYN\n",
    "        if 'adasyn' in techniques:\n",
    "            print(\"\\nApplying ADASYN...\")\n",
    "            adasyn = ADASYN(random_state=self.random_state)\n",
    "            X_train_adasyn, y_train_adasyn = adasyn.fit_resample(X_train_processed, y_train)\n",
    "            \n",
    "            adasyn_counts = np.bincount(y_train_adasyn)\n",
    "            adasyn_ratio = adasyn_counts[1] / len(y_train_adasyn)\n",
    "            print(f\"ADASYN class distribution: {adasyn_counts}, ratio = {adasyn_ratio:.2f}\")\n",
    "            \n",
    "            results['adasyn'] = (X_train_adasyn, y_train_adasyn, preprocessor)\n",
    "        \n",
    "        # Class weights\n",
    "        if 'class_weights' in techniques:\n",
    "            print(\"\\nCalculating class weights...\")\n",
    "            # Compute class weights inversely proportional to class frequencies\n",
    "            class_weights = {\n",
    "                0: len(y_train) / (2 * original_counts[0]),\n",
    "                1: len(y_train) / (2 * original_counts[1])\n",
    "            }\n",
    "            print(f\"Class weights: {class_weights}\")\n",
    "            \n",
    "            # Store class weights for later use with models\n",
    "            self.class_weights = class_weights\n",
    "        \n",
    "        # Store results\n",
    "        self.imbalance_handling_results = results\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def _create_lightgbm_model(self, params=None):\n",
    "        \"\"\"\n",
    "        Create a LightGBM classifier with the given parameters.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        params : dict\n",
    "            Model parameters\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        model : LGBMClassifier\n",
    "            LightGBM classifier\n",
    "        \"\"\"\n",
    "        default_params = {\n",
    "            'objective': 'binary',\n",
    "            'metric': 'auc',\n",
    "            'n_estimators': 200,\n",
    "            'learning_rate': 0.05,\n",
    "            'num_leaves': 31,\n",
    "            'max_depth': -1,\n",
    "            'min_child_samples': 20,\n",
    "            'subsample': 0.8,\n",
    "            'colsample_bytree': 0.8,\n",
    "            'reg_alpha': 0.1,\n",
    "            'reg_lambda': 0.1,\n",
    "            'random_state': self.random_state,\n",
    "            'class_weight': 'balanced',\n",
    "            'n_jobs': -1\n",
    "        }\n",
    "        \n",
    "        if params is not None:\n",
    "            default_params.update(params)\n",
    "        \n",
    "        return lgb.LGBMClassifier(**default_params)\n",
    "    \n",
    "    def _create_xgboost_model(self, params=None):\n",
    "        \"\"\"\n",
    "        Create an XGBoost classifier with the given parameters.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        params : dict\n",
    "            Model parameters\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        model : XGBClassifier\n",
    "            XGBoost classifier\n",
    "        \"\"\"\n",
    "        default_params = {\n",
    "            'objective': 'binary:logistic',\n",
    "            'eval_metric': 'auc',\n",
    "            'n_estimators': 200,\n",
    "            'learning_rate': 0.05,\n",
    "            'max_depth': 6,\n",
    "            'min_child_weight': 1,\n",
    "            'subsample': 0.8,\n",
    "            'colsample_bytree': 0.8,\n",
    "            'gamma': 0.1,\n",
    "            'reg_alpha': 0.1,\n",
    "            'reg_lambda': 0.1,\n",
    "            'scale_pos_weight': 1,\n",
    "            'random_state': self.random_state,\n",
    "            'n_jobs': -1\n",
    "        }\n",
    "        \n",
    "        if params is not None:\n",
    "            default_params.update(params)\n",
    "        \n",
    "        return xgb.XGBClassifier(**default_params)\n",
    "    \n",
    "    def _create_catboost_model(self, params=None):\n",
    "        \"\"\"\n",
    "        Create a CatBoost classifier with the given parameters.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        params : dict\n",
    "            Model parameters\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        model : CatBoostClassifier\n",
    "            CatBoost classifier\n",
    "        \"\"\"\n",
    "        default_params = {\n",
    "            'iterations': 200,\n",
    "            'learning_rate': 0.05,\n",
    "            'depth': 6,\n",
    "            'l2_leaf_reg': 3,\n",
    "            'bagging_temperature': 1,\n",
    "            'random_strength': 1,\n",
    "            'od_type': 'Iter',\n",
    "            'od_wait': 50,\n",
    "            'random_seed': self.random_state,\n",
    "            'verbose': 0,\n",
    "            'task_type': 'CPU',\n",
    "            'loss_function': 'Logloss',\n",
    "            'eval_metric': 'AUC',\n",
    "            'class_weights': [1, 2]  # Adjust for class imbalance\n",
    "        }\n",
    "        \n",
    "        if params is not None:\n",
    "            default_params.update(params)\n",
    "        \n",
    "        return cb.CatBoostClassifier(**default_params)\n",
    "\n",
    "    def train_models(self, X_train, y_train, X_val, y_val, techniques=None):\n",
    "        \"\"\"\n",
    "        Train multiple models on the dataset.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        X_train : numpy.ndarray or pandas.DataFrame\n",
    "            Training features\n",
    "        y_train : numpy.ndarray or pandas.Series\n",
    "            Training target\n",
    "        X_val : numpy.ndarray or pandas.DataFrame\n",
    "            Validation features\n",
    "        y_val : numpy.ndarray or pandas.Series\n",
    "            Validation target\n",
    "        techniques : list\n",
    "            List of resampling techniques to use\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        model_results : dict\n",
    "            Dictionary with model results\n",
    "        \"\"\"\n",
    "        print(\"\\nTraining multiple models...\")\n",
    "        \n",
    "        if techniques is None:\n",
    "            techniques = ['original', 'smote', 'smote_tomek', 'adasyn']\n",
    "        \n",
    "        # Store results\n",
    "        model_results = {}\n",
    "        \n",
    "        # Loop over each balancing technique\n",
    "        for technique in techniques:\n",
    "            print(f\"\\nTraining models with {technique} balancing...\")\n",
    "            \n",
    "            # Get the appropriate data\n",
    "            if technique in self.imbalance_handling_results:\n",
    "                X_technique, y_technique, preprocessor = self.imbalance_handling_results[technique]\n",
    "                \n",
    "                # If not original, data is already preprocessed\n",
    "                is_preprocessed = technique != 'original'\n",
    "            else:\n",
    "                print(f\"Technique {technique} not found in imbalance handling results. Using original data.\")\n",
    "                X_technique, y_technique = X_train, y_train\n",
    "                preprocessor = self._create_preprocessor()\n",
    "                is_preprocessed = False\n",
    "            \n",
    "            # Preprocess validation data\n",
    "            X_val_processed = preprocessor.transform(X_val) if not is_preprocessed else X_val\n",
    "            \n",
    "            # Preprocess training data if needed\n",
    "            if not is_preprocessed:\n",
    "                X_technique = preprocessor.fit_transform(X_technique)\n",
    "            \n",
    "            # Store models for this technique\n",
    "            technique_models = {}\n",
    "            \n",
    "            # 1. Logistic Regression\n",
    "            print(\"\\nTraining Logistic Regression...\")\n",
    "            lr = LogisticRegression(\n",
    "                C=1.0,\n",
    "                penalty='l2',\n",
    "                solver='liblinear',\n",
    "                class_weight='balanced',\n",
    "                random_state=self.random_state,\n",
    "                max_iter=1000\n",
    "            )\n",
    "            lr.fit(X_technique, y_technique)\n",
    "            \n",
    "            # Evaluate\n",
    "            lr_val_pred = lr.predict_proba(X_val_processed)[:, 1]\n",
    "            lr_val_auc = roc_auc_score(y_val, lr_val_pred)\n",
    "            print(f\"Logistic Regression Validation AUC: {lr_val_auc:.4f}\")\n",
    "            \n",
    "            technique_models['logistic_regression'] = {\n",
    "                'model': lr,\n",
    "                'val_auc': lr_val_auc,\n",
    "                'preprocessor': preprocessor,\n",
    "                'is_preprocessed': is_preprocessed\n",
    "            }\n",
    "            \n",
    "            # 2. Random Forest\n",
    "            print(\"\\nTraining Random Forest...\")\n",
    "            rf = RandomForestClassifier(\n",
    "                n_estimators=200,\n",
    "                max_depth=10,\n",
    "                min_samples_split=10,\n",
    "                min_samples_leaf=4,\n",
    "                max_features='sqrt',\n",
    "                bootstrap=True,\n",
    "                class_weight='balanced',\n",
    "                random_state=self.random_state,\n",
    "                n_jobs=-1\n",
    "            )\n",
    "            rf.fit(X_technique, y_technique)\n",
    "            \n",
    "            # Evaluate\n",
    "            rf_val_pred = rf.predict_proba(X_val_processed)[:, 1]\n",
    "            rf_val_auc = roc_auc_score(y_val, rf_val_pred)\n",
    "            print(f\"Random Forest Validation AUC: {rf_val_auc:.4f}\")\n",
    "            \n",
    "            technique_models['random_forest'] = {\n",
    "                'model': rf,\n",
    "                'val_auc': rf_val_auc,\n",
    "                'preprocessor': preprocessor,\n",
    "                'is_preprocessed': is_preprocessed\n",
    "            }\n",
    "            \n",
    "            # 3. XGBoost\n",
    "            print(\"\\nTraining XGBoost...\")\n",
    "            xgb_model = self._create_xgboost_model()\n",
    "            xgb_model.fit(\n",
    "                X_technique, y_technique,\n",
    "                eval_set=[(X_val_processed, y_val)],\n",
    "                early_stopping_rounds=50,\n",
    "                verbose=False\n",
    "            )\n",
    "            \n",
    "            # Evaluate\n",
    "            xgb_val_pred = xgb_model.predict_proba(X_val_processed)[:, 1]\n",
    "            xgb_val_auc = roc_auc_score(y_val, xgb_val_pred)\n",
    "            print(f\"XGBoost Validation AUC: {xgb_val_auc:.4f}\")\n",
    "            \n",
    "            technique_models['xgboost'] = {\n",
    "                'model': xgb_model,\n",
    "                'val_auc': xgb_val_auc,\n",
    "                'preprocessor': preprocessor,\n",
    "                'is_preprocessed': is_preprocessed\n",
    "            }\n",
    "            \n",
    "            # 4. LightGBM\n",
    "            print(\"\\nTraining LightGBM...\")\n",
    "            lgb_model = self._create_lightgbm_model()\n",
    "            lgb_model.fit(\n",
    "                X_technique, y_technique,\n",
    "                eval_set=[(X_val_processed, y_val)],\n",
    "                early_stopping_rounds=50,\n",
    "                verbose=False\n",
    "            )\n",
    "            \n",
    "            # Evaluate\n",
    "            lgb_val_pred = lgb_model.predict_proba(X_val_processed)[:, 1]\n",
    "            lgb_val_auc = roc_auc_score(y_val, lgb_val_pred)\n",
    "            print(f\"LightGBM Validation AUC: {lgb_val_auc:.4f}\")\n",
    "            \n",
    "            technique_models['lightgbm'] = {\n",
    "                'model': lgb_model,\n",
    "                'val_auc': lgb_val_auc,\n",
    "                'preprocessor': preprocessor,\n",
    "                'is_preprocessed': is_preprocessed\n",
    "            }\n",
    "            \n",
    "            # 5. CatBoost\n",
    "            print(\"\\nTraining CatBoost...\")\n",
    "            cb_model = self._create_catboost_model()\n",
    "            cb_model.fit(\n",
    "                X_technique, y_technique,\n",
    "                eval_set=(X_val_processed, y_val),\n",
    "                early_stopping_rounds=50,\n",
    "                verbose=False\n",
    "            )\n",
    "            \n",
    "            # Evaluate\n",
    "            cb_val_pred = cb_model.predict_proba(X_val_processed)[:, 1]\n",
    "            cb_val_auc = roc_auc_score(y_val, cb_val_pred)\n",
    "            print(f\"CatBoost Validation AUC: {cb_val_auc:.4f}\")\n",
    "            \n",
    "            technique_models['catboost'] = {\n",
    "                'model': cb_model,\n",
    "                'val_auc': cb_val_auc,\n",
    "                'preprocessor': preprocessor,\n",
    "                'is_preprocessed': is_preprocessed\n",
    "            }\n",
    "            \n",
    "            # Store all models for this technique\n",
    "            model_results[technique] = technique_models\n",
    "        \n",
    "        # Find the best model\n",
    "        best_model_info = {'val_auc': 0, 'model_name': None, 'technique': None}\n",
    "        \n",
    "        for technique, models in model_results.items():\n",
    "            for model_name, model_info in models.items():\n",
    "                if model_info['val_auc'] > best_model_info['val_auc']:\n",
    "                    best_model_info['val_auc'] = model_info['val_auc']\n",
    "                    best_model_info['model_name'] = model_name\n",
    "                    best_model_info['technique'] = technique\n",
    "        \n",
    "        print(f\"\\nBest model: {best_model_info['model_name']} with {best_model_info['technique']} balancing\")\n",
    "        print(f\"Validation AUC: {best_model_info['val_auc']:.4f}\")\n",
    "        \n",
    "        # Store the best model information\n",
    "        self.best_model_info = best_model_info\n",
    "        self.model_results = model_results\n",
    "        \n",
    "        return model_results\n",
    "    \n",
    "    def hyperparameter_tuning(self, X_train, y_train, X_val, y_val, model_name, technique, n_trials=50):\n",
    "        \"\"\"\n",
    "        Perform hyperparameter tuning for a specific model.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        X_train : numpy.ndarray or pandas.DataFrame\n",
    "            Training features\n",
    "        y_train : numpy.ndarray or pandas.Series\n",
    "            Training target\n",
    "        X_val : numpy.ndarray or pandas.Series\n",
    "            Validation features\n",
    "        y_val : numpy.ndarray or pandas.Series\n",
    "            Validation target\n",
    "        model_name : str\n",
    "            Name of the model to tune\n",
    "        technique : str\n",
    "            Resampling technique to use\n",
    "        n_trials : int\n",
    "            Number of trials for hyperparameter optimization\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        best_params : dict\n",
    "            Best hyperparameters found\n",
    "        best_model : object\n",
    "            Best model with tuned hyperparameters\n",
    "        \"\"\"\n",
    "        print(f\"\\nPerforming hyperparameter tuning for {model_name} with {technique} balancing...\")\n",
    "        \n",
    "        # Get the appropriate data\n",
    "        if technique in self.imbalance_handling_results:\n",
    "            X_technique, y_technique, preprocessor = self.imbalance_handling_results[technique]\n",
    "            \n",
    "            # If not original, data is already preprocessed\n",
    "            is_preprocessed = technique != 'original'\n",
    "        else:\n",
    "            print(f\"Technique {technique} not found in imbalance handling results. Using original data.\")\n",
    "            X_technique, y_technique = X_train, y_train\n",
    "            preprocessor = self._create_preprocessor()\n",
    "            is_preprocessed = False\n",
    "        \n",
    "        # Preprocess validation data\n",
    "        X_val_processed = preprocessor.transform(X_val) if not is_preprocessed else X_val\n",
    "        \n",
    "        # Preprocess training data if needed\n",
    "        if not is_preprocessed:\n",
    "            X_technique = preprocessor.fit_transform(X_technique)\n",
    "        \n",
    "        # Define the objective function for Optuna\n",
    "        def objective(trial):\n",
    "            # Different parameter space for each model\n",
    "            if model_name == 'xgboost':\n",
    "                params = {\n",
    "                    'n_estimators': trial.suggest_int('n_estimators', 100, 500),\n",
    "                    'max_depth': trial.suggest_int('max_depth', 3, 12),\n",
    "                    'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3, log=True),\n",
    "                    'subsample': trial.suggest_float('subsample', 0.6, 1.0),\n",
    "                    'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 1.0),\n",
    "                    'min_child_weight': trial.suggest_int('min_child_weight', 1, 10),\n",
    "                    'gamma': trial.suggest_float('gamma', 0.0, 1.0),\n",
    "                    'reg_alpha': trial.suggest_float('reg_alpha', 0.0, 10.0, log=True),\n",
    "                    'reg_lambda': trial.suggest_float('reg_lambda', 0.0, 10.0, log=True),\n",
    "                    'scale_pos_weight': trial.suggest_float('scale_pos_weight', 1.0, 10.0)\n",
    "                }\n",
    "                \n",
    "                model = self._create_xgboost_model(params)\n",
    "                \n",
    "            elif model_name == 'lightgbm':\n",
    "                params = {\n",
    "                    'n_estimators': trial.suggest_int('n_estimators', 100, 500),\n",
    "                    'num_leaves': trial.suggest_int('num_leaves', 20, 150),\n",
    "                    'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3, log=True),\n",
    "                    'max_depth': trial.suggest_int('max_depth', -1, 12),\n",
    "                    'min_child_samples': trial.suggest_int('min_child_samples', 10, 100),\n",
    "                    'subsample': trial.suggest_float('subsample', 0.6, 1.0),\n",
    "                    'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 1.0),\n",
    "                    'reg_alpha': trial.suggest_float('reg_alpha', 0.0, 10.0, log=True),\n",
    "                    'reg_lambda': trial.suggest_float('reg_lambda', 0.0, 10.0, log=True)\n",
    "                }\n",
    "                \n",
    "                model = self._create_lightgbm_model(params)\n",
    "                \n",
    "            elif model_name == 'catboost':\n",
    "                params = {\n",
    "                    'iterations': trial.suggest_int('iterations', 100, 500),\n",
    "                    'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3, log=True),\n",
    "                    'depth': trial.suggest_int('depth', 4, 10),\n",
    "                    'l2_leaf_reg': trial.suggest_float('l2_leaf_reg', 1.0, 10.0),\n",
    "                    'bagging_temperature': trial.suggest_float('bagging_temperature', 0.0, 10.0),\n",
    "                    'random_strength': trial.suggest_float('random_strength', 0.0, 10.0)\n",
    "                }\n",
    "                \n",
    "                model = self._create_catboost_model(params)\n",
    "                \n",
    "            elif model_name == 'random_forest':\n",
    "                params = {\n",
    "                    'n_estimators': trial.suggest_int('n_estimators', 100, 500),\n",
    "                    'max_depth': trial.suggest_int('max_depth', 5, 30),\n",
    "                    'min_samples_split': trial.suggest_int('min_samples_split', 2, 20),\n",
    "                    'min_samples_leaf': trial.suggest_int('min_samples_leaf', 1, 10),\n",
    "                    'max_features': trial.suggest_categorical('max_features', ['sqrt', 'log2'])\n",
    "                }\n",
    "                \n",
    "                model = RandomForestClassifier(\n",
    "                    **params,\n",
    "                    class_weight='balanced',\n",
    "                    random_state=self.random_state,\n",
    "                    n_jobs=-1\n",
    "                )\n",
    "                \n",
    "            elif model_name == 'logistic_regression':\n",
    "                params = {\n",
    "                    'C': trial.suggest_float('C', 0.001, 10.0, log=True),\n",
    "                    'penalty': trial.suggest_categorical('penalty', ['l1', 'l2']),\n",
    "                    'solver': trial.suggest_categorical('solver', ['liblinear', 'saga'])\n",
    "                }\n",
    "                \n",
    "                model = LogisticRegression(\n",
    "                    **params,\n",
    "                    class_weight='balanced',\n",
    "                    random_state=self.random_state,\n",
    "                    max_iter=1000\n",
    "                )\n",
    "                \n",
    "            else:\n",
    "                raise ValueError(f\"Unsupported model: {model_name}\")\n",
    "            \n",
    "            # Train the model\n",
    "            if model_name in ['xgboost', 'lightgbm', 'catboost']:\n",
    "                model.fit(\n",
    "                    X_technique, y_technique,\n",
    "                    eval_set=[(X_val_processed, y_val)],\n",
    "                    early_stopping_rounds=50,\n",
    "                    verbose=False\n",
    "                )\n",
    "            else:\n",
    "                model.fit(X_technique, y_technique)\n",
    "            \n",
    "            # Make predictions\n",
    "            if hasattr(model, 'predict_proba'):\n",
    "                y_val_pred = model.predict_proba(X_val_processed)[:, 1]\n",
    "                val_auc = roc_auc_score(y_val, y_val_pred)\n",
    "            else:\n",
    "                y_val_pred = model.predict(X_val_processed)\n",
    "                val_auc = accuracy_score(y_val, y_val_pred)\n",
    "            \n",
    "            return val_auc\n",
    "        \n",
    "        # Create Optuna study\n",
    "        study = optuna.create_study(direction='maximize')\n",
    "        study.optimize(objective, n_trials=n_trials)\n",
    "        \n",
    "        print(f\"Best parameters: {study.best_params}\")\n",
    "        print(f\"Best validation AUC: {study.best_value:.4f}\")\n",
    "        \n",
    "        # Train the best model\n",
    "        if model_name == 'xgboost':\n",
    "            best_model = self._create_xgboost_model(study.best_params)\n",
    "        elif model_name == 'lightgbm':\n",
    "            best_model = self._create_lightgbm_model(study.best_params)\n",
    "        elif model_name == 'catboost':\n",
    "            best_model = self._create_catboost_model(study.best_params)\n",
    "        elif model_name == 'random_forest':\n",
    "            best_model = RandomForestClassifier(\n",
    "                **study.best_params,\n",
    "                class_weight='balanced',\n",
    "                random_state=self.random_state,\n",
    "                n_jobs=-1\n",
    "            )\n",
    "        elif model_name == 'logistic_regression':\n",
    "            best_model = LogisticRegression(\n",
    "                **study.best_params,\n",
    "                class_weight='balanced',\n",
    "                random_state=self.random_state,\n",
    "                max_iter=1000\n",
    "            )\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported model: {model_name}\")\n",
    "        \n",
    "        # Fit the best model\n",
    "        if model_name in ['xgboost', 'lightgbm', 'catboost']:\n",
    "            best_model.fit(\n",
    "                X_technique, y_technique,\n",
    "                eval_set=[(X_val_processed, y_val)],\n",
    "                early_stopping_rounds=50,\n",
    "                verbose=False\n",
    "            )\n",
    "        else:\n",
    "            best_model.fit(X_technique, y_technique)\n",
    "        \n",
    "        # Store the best model\n",
    "        self.best_tuned_model = {\n",
    "            'model': best_model,\n",
    "            'params': study.best_params,\n",
    "            'val_auc': study.best_value,\n",
    "            'preprocessor': preprocessor,\n",
    "            'is_preprocessed': is_preprocessed\n",
    "        }\n",
    "        \n",
    "        return study.best_params, best_model\n",
    "    \n",
    "    def create_ensemble(self, X_train, y_train, X_val, y_val, top_n=3):\n",
    "        \"\"\"\n",
    "        Create an ensemble of the best models.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        X_train : numpy.ndarray or pandas.DataFrame\n",
    "            Training features\n",
    "        y_train : numpy.ndarray or pandas.Series\n",
    "            Training target\n",
    "        X_val : numpy.ndarray or pandas.DataFrame\n",
    "            Validation features\n",
    "        y_val : numpy.ndarray or pandas.Series\n",
    "            Validation target\n",
    "        top_n : int\n",
    "            Number of top models to include in the ensemble\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        ensemble_model : object\n",
    "            Ensemble model\n",
    "        \"\"\"\n",
    "        print(f\"\\nCreating an ensemble of top {top_n} models...\")\n",
    "        \n",
    "        # Collect all models and their validation AUCs\n",
    "        all_models = []\n",
    "        \n",
    "        for technique, models in self.model_results.items():\n",
    "            for model_name, model_info in models.items():\n",
    "                all_models.append({\n",
    "                    'technique': technique,\n",
    "                    'model_name': model_name,\n",
    "                    'val_auc': model_info['val_auc'],\n",
    "                    'model': model_info['model'],\n",
    "                    'preprocessor': model_info['preprocessor'],\n",
    "                    'is_preprocessed': model_info['is_preprocessed']\n",
    "                })\n",
    "        \n",
    "        # Sort by validation AUC\n",
    "        all_models.sort(key=lambda x: x['val_auc'], reverse=True)\n",
    "        \n",
    "        # Select top N models\n",
    "        top_models = all_models[:top_n]\n",
    "        \n",
    "        print(\"\\nTop models selected for ensemble:\")\n",
    "        for i, model_info in enumerate(top_models, 1):\n",
    "            print(f\"{i}. {model_info['model_name']} with {model_info['technique']} balancing, AUC = {model_info['val_auc']:.4f}\")\n",
    "        \n",
    "        # 1. Voting Ensemble\n",
    "        print(\"\\nCreating Voting Classifier...\")\n",
    "        \n",
    "        estimators = []\n",
    "        for i, model_info in enumerate(top_models):\n",
    "            estimator_name = f\"{model_info['model_name']}_{i}\"\n",
    "            estimators.append((estimator_name, model_info['model']))\n",
    "        \n",
    "        voting_ensemble = VotingClassifier(\n",
    "            estimators=estimators,\n",
    "            voting='soft',\n",
    "            n_jobs=-1\n",
    "        )\n",
    "        \n",
    "        # We need to fit the voting ensemble with preprocessed data\n",
    "        # Preprocess training data with each model's preprocessor\n",
    "        X_train_processed_dict = {}\n",
    "        X_val_processed_dict = {}\n",
    "        \n",
    "        for model_info in top_models:\n",
    "            technique = model_info['technique']\n",
    "            preprocessor = model_info['preprocessor']\n",
    "            is_preprocessed = model_info['is_preprocessed']\n",
    "            \n",
    "            # Get or preprocess the training data\n",
    "            if technique in self.imbalance_handling_results:\n",
    "                X_technique, y_technique, _ = self.imbalance_handling_results[technique]\n",
    "                \n",
    "                # If not original, data is already preprocessed\n",
    "                if not is_preprocessed:\n",
    "                    X_technique = preprocessor.transform(X_technique)\n",
    "            else:\n",
    "                X_technique = preprocessor.transform(X_train)\n",
    "                y_technique = y_train\n",
    "            \n",
    "            # Preprocess validation data\n",
    "            X_val_processed = preprocessor.transform(X_val)\n",
    "            \n",
    "            # Store processed data\n",
    "            X_train_processed_dict[technique] = (X_technique, y_technique)\n",
    "            X_val_processed_dict[technique] = X_val_processed\n",
    "        \n",
    "        # We can only fit the voting ensemble with one set of data\n",
    "        # Let's use the data from the best model\n",
    "        best_technique = top_models[0]['technique']\n",
    "        X_train_processed, y_train_processed = X_train_processed_dict[best_technique]\n",
    "        \n",
    "        # Fit the voting ensemble\n",
    "        voting_ensemble.fit(X_train_processed, y_train_processed)\n",
    "        \n",
    "        # Evaluate the voting ensemble\n",
    "        X_val_processed = X_val_processed_dict[best_technique]\n",
    "        voting_val_pred = voting_ensemble.predict_proba(X_val_processed)[:, 1]\n",
    "        voting_val_auc = roc_auc_score(y_val, voting_val_pred)\n",
    "        \n",
    "        print(f\"Voting Ensemble Validation AUC: {voting_val_auc:.4f}\")\n",
    "        \n",
    "        # 2. Stacking Ensemble\n",
    "        print(\"\\nCreating Stacking Classifier...\")\n",
    "        \n",
    "        # Define base models\n",
    "        base_estimators = []\n",
    "        for i, model_info in enumerate(top_models):\n",
    "            estimator_name = f\"{model_info['model_name']}_{i}\"\n",
    "            base_estimators.append((estimator_name, model_info['model']))\n",
    "        \n",
    "        # Define meta-learner\n",
    "        meta_learner = LogisticRegression(max_iter=1000)\n",
    "        \n",
    "        # Create stacking ensemble\n",
    "        stacking_ensemble = StackingClassifier(\n",
    "            estimators=base_estimators,\n",
    "            final_estimator=meta_learner,\n",
    "            cv=5,\n",
    "            n_jobs=-1\n",
    "        )\n",
    "        \n",
    "        # Fit stacking ensemble\n",
    "        stacking_ensemble.fit(X_train_processed, y_train_processed)\n",
    "        \n",
    "        # Evaluate stacking ensemble\n",
    "        stacking_val_pred = stacking_ensemble.predict_proba(X_val_processed)[:, 1]\n",
    "        stacking_val_auc = roc_auc_score(y_val, stacking_val_pred)\n",
    "        \n",
    "        print(f\"Stacking Ensemble Validation AUC: {stacking_val_auc:.4f}\")\n",
    "        \n",
    "        # 3. Weighted Average Ensemble\n",
    "        print(\"\\nCreating Weighted Average Ensemble...\")\n",
    "        \n",
    "        # Create a simple weighted average function\n",
    "        def weighted_average_ensemble(X, models, weights, preprocessors):\n",
    "            \"\"\"\n",
    "            Make predictions using weighted average of base models.\n",
    "            \"\"\"\n",
    "            predictions = []\n",
    "            \n",
    "            for model, weight, preprocessor in zip(models, weights, preprocessors):\n",
    "                # Preprocess the data for this model\n",
    "                X_processed = preprocessor.transform(X)\n",
    "                \n",
    "                # Get predictions\n",
    "                pred = model.predict_proba(X_processed)[:, 1]\n",
    "                predictions.append(pred * weight)\n",
    "            \n",
    "            # Weighted average\n",
    "            weighted_pred = sum(predictions) / sum(weights)\n",
    "            \n",
    "            return weighted_pred\n",
    "        \n",
    "        # Extract models, weights (based on validation AUC), and preprocessors\n",
    "        models = [model_info['model'] for model_info in top_models]\n",
    "        weights = [model_info['val_auc'] for model_info in top_models]\n",
    "        preprocessors = [model_info['preprocessor'] for model_info in top_models]\n",
    "        \n",
    "        # Make predictions\n",
    "        weighted_val_pred = weighted_average_ensemble(X_val, models, weights, preprocessors)\n",
    "        weighted_val_auc = roc_auc_score(y_val, weighted_val_pred)\n",
    "        \n",
    "        print(f\"Weighted Average Ensemble Validation AUC: {weighted_val_auc:.4f}\")\n",
    "        \n",
    "        # Compare ensemble methods\n",
    "        ensemble_results = {\n",
    "            'voting': {\n",
    "                'ensemble': voting_ensemble,\n",
    "                'val_auc': voting_val_auc,\n",
    "                'preprocessor': top_models[0]['preprocessor']\n",
    "            },\n",
    "            'stacking': {\n",
    "                'ensemble': stacking_ensemble,\n",
    "                'val_auc': stacking_val_auc,\n",
    "                'preprocessor': top_models[0]['preprocessor']\n",
    "            },\n",
    "            'weighted': {\n",
    "                'ensemble': {\n",
    "                    'models': models,\n",
    "                    'weights': weights,\n",
    "                    'preprocessors': preprocessors\n",
    "                },\n",
    "                'val_auc': weighted_val_auc,\n",
    "                'prediction_function': weighted_average_ensemble\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        # Find the best ensemble\n",
    "        best_ensemble = max(ensemble_results.items(), key=lambda x: x[1]['val_auc'])\n",
    "        best_ensemble_name, best_ensemble_info = best_ensemble\n",
    "        \n",
    "        print(f\"\\nBest ensemble method: {best_ensemble_name}\")\n",
    "        print(f\"Best ensemble validation AUC: {best_ensemble_info['val_auc']:.4f}\")\n",
    "        \n",
    "        # Store ensemble results\n",
    "        self.ensemble_results = ensemble_results\n",
    "        self.best_ensemble = best_ensemble\n",
    "        \n",
    "        return ensemble_results\n",
    "    \n",
    "    def evaluate_model(self, model, X_test, y_test, preprocessor=None, is_ensemble=False, ensemble_type=None):\n",
    "        \"\"\"\n",
    "        Evaluate a model on the test set.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        model : object\n",
    "            Model to evaluate\n",
    "        X_test : pandas.DataFrame\n",
    "            Test features\n",
    "        y_test : pandas.Series\n",
    "            Test target\n",
    "        preprocessor : object\n",
    "            Preprocessor for the data\n",
    "        is_ensemble : bool\n",
    "            Whether the model is an ensemble\n",
    "        ensemble_type : str\n",
    "            Type of ensemble ('voting', 'stacking', 'weighted')\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        metrics : dict\n",
    "            Dictionary with evaluation metrics\n",
    "        \"\"\"\n",
    "        print(\"\\nEvaluating model on test set...\")\n",
    "        \n",
    "        # Preprocess the test data\n",
    "        if preprocessor is not None:\n",
    "            X_test_processed = preprocessor.transform(X_test)\n",
    "        else:\n",
    "            X_test_processed = X_test\n",
    "        \n",
    "        # Make predictions\n",
    "        if is_ensemble and ensemble_type == 'weighted':\n",
    "            # For weighted ensemble, use the prediction function\n",
    "            models = model['models']\n",
    "            weights = model['weights']\n",
    "            preprocessors = model['preprocessors']\n",
    "            \n",
    "            y_pred_proba = self.ensemble_results['weighted']['prediction_function'](X_test, models, weights, preprocessors)\n",
    "            y_pred = (y_pred_proba > 0.5).astype(int)\n",
    "        else:\n",
    "            # For regular models and other ensembles\n",
    "            y_pred_proba = model.predict_proba(X_test_processed)[:, 1]\n",
    "            y_pred = model.predict(X_test_processed)\n",
    "        \n",
    "        # Calculate metrics\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        precision = precision_score(y_test, y_pred)\n",
    "        recall = recall_score(y_test, y_pred)\n",
    "        f1 = f1_score(y_test, y_pred)\n",
    "        auc_roc = roc_auc_score(y_test, y_pred_proba)\n",
    "        logloss = log_loss(y_test, y_pred_proba)\n",
    "        \n",
    "        # Confusion matrix\n",
    "        cm = confusion_matrix(y_test, y_pred)\n",
    "        \n",
    "        # Classification report\n",
    "        cls_report = classification_report(y_test, y_pred)\n",
    "        \n",
    "        # Print results\n",
    "        print(f\"Accuracy: {accuracy:.4f}\")\n",
    "        print(f\"Precision: {precision:.4f}\")\n",
    "        print(f\"Recall: {recall:.4f}\")\n",
    "        print(f\"F1 Score: {f1:.4f}\")\n",
    "        print(f\"AUC-ROC: {auc_roc:.4f}\")\n",
    "        print(f\"Log Loss: {logloss:.4f}\")\n",
    "        \n",
    "        print(\"\\nConfusion Matrix:\")\n",
    "        print(cm)\n",
    "        \n",
    "        print(\"\\nClassification Report:\")\n",
    "        print(cls_report)\n",
    "        \n",
    "        # Store results\n",
    "        metrics = {\n",
    "            'accuracy': accuracy,\n",
    "            'precision': precision,\n",
    "            'recall': recall,\n",
    "            'f1': f1,\n",
    "            'auc_roc': auc_roc,\n",
    "            'log_loss': logloss,\n",
    "            'confusion_matrix': cm,\n",
    "            'classification_report': cls_report\n",
    "        }\n",
    "        \n",
    "        return metrics\n",
    "    \n",
    "    def interpret_model(self, model, X_test, preprocessor=None, is_ensemble=False, n_features=20):\n",
    "        \"\"\"\n",
    "        Interpret the model using SHAP values.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        model : object\n",
    "            Model to interpret\n",
    "        X_test : pandas.DataFrame\n",
    "            Test features\n",
    "        preprocessor : object\n",
    "            Preprocessor for the data\n",
    "        is_ensemble : bool\n",
    "            Whether the model is an ensemble\n",
    "        n_features : int\n",
    "            Number of top features to display\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        feature_importance : pandas.DataFrame\n",
    "            Feature importance dataframe\n",
    "        \"\"\"\n",
    "        print(\"\\nInterpreting model using SHAP values...\")\n",
    "        \n",
    "        # Preprocess the test data\n",
    "        if preprocessor is not None:\n",
    "            X_test_processed = preprocessor.transform(X_test)\n",
    "        else:\n",
    "            X_test_processed = X_test\n",
    "        \n",
    "        # Get feature names\n",
    "        if hasattr(preprocessor, 'get_feature_names_out'):\n",
    "            try:\n",
    "                feature_names = preprocessor.get_feature_names_out()\n",
    "            except:\n",
    "                # Fallback to generic feature names\n",
    "                feature_names = [f\"feature_{i}\" for i in range(X_test_processed.shape[1])]\n",
    "        else:\n",
    "            feature_names = [f\"feature_{i}\" for i in range(X_test_processed.shape[1])]\n",
    "        \n",
    "        # For tree-based models, use TreeExplainer\n",
    "        if not is_ensemble and hasattr(model, 'feature_importances_'):\n",
    "            print(\"Using TreeExplainer for SHAP values...\")\n",
    "            \n",
    "            # Create explainer\n",
    "            explainer = shap.TreeExplainer(model)\n",
    "            \n",
    "            # Get SHAP values\n",
    "            shap_values = explainer.shap_values(X_test_processed)\n",
    "            \n",
    "            # If shap_values is a list, get the values for the positive class\n",
    "            if isinstance(shap_values, list):\n",
    "                shap_values = shap_values[1]\n",
    "            \n",
    "            # Create SHAP summary plot\n",
    "            print(\"\\nSHAP Summary Plot:\")\n",
    "            print(\"(This would display a visual summary of feature impacts)\")\n",
    "            \n",
    "            # Get feature importance from SHAP values\n",
    "            feature_importance = pd.DataFrame({\n",
    "                'Feature': feature_names,\n",
    "                'Importance': np.abs(shap_values).mean(axis=0)\n",
    "            }).sort_values('Importance', ascending=False).head(n_features)\n",
    "            \n",
    "        # For other models, use a different approach\n",
    "        else:\n",
    "            print(\"Using permutation importance for feature importance...\")\n",
    "            \n",
    "            from sklearn.inspection import permutation_importance\n",
    "            \n",
    "            # Calculate permutation importance\n",
    "            if not is_ensemble:\n",
    "                perm_importance = permutation_importance(\n",
    "                    model, X_test_processed, y_test,\n",
    "                    n_repeats=10,\n",
    "                    random_state=self.random_state,\n",
    "                    n_jobs=-1\n",
    "                )\n",
    "                \n",
    "                # Get feature importance\n",
    "                feature_importance = pd.DataFrame({\n",
    "                    'Feature': feature_names,\n",
    "                    'Importance': perm_importance.importances_mean\n",
    "                }).sort_values('Importance', ascending=False).head(n_features)\n",
    "            else:\n",
    "                # For ensembles, average feature importance from base models\n",
    "                if hasattr(model, 'estimators_'):\n",
    "                    importances = []\n",
    "                    \n",
    "                    for estimator in model.estimators_:\n",
    "                        if hasattr(estimator, 'feature_importances_'):\n",
    "                            importances.append(estimator.feature_importances_)\n",
    "                    \n",
    "                    if importances:\n",
    "                        avg_importance = np.mean(importances, axis=0)\n",
    "                        \n",
    "                        feature_importance = pd.DataFrame({\n",
    "                            'Feature': feature_names,\n",
    "                            'Importance': avg_importance\n",
    "                        }).sort_values('Importance', ascending=False).head(n_features)\n",
    "                    else:\n",
    "                        feature_importance = pd.DataFrame({\n",
    "                            'Feature': ['N/A'],\n",
    "                            'Importance': [0]\n",
    "                        })\n",
    "                else:\n",
    "                    feature_importance = pd.DataFrame({\n",
    "                        'Feature': ['N/A'],\n",
    "                        'Importance': [0]\n",
    "                    })\n",
    "        \n",
    "        # Print top features\n",
    "        print(\"\\nTop features by importance:\")\n",
    "        print(feature_importance)\n",
    "        \n",
    "        # Store feature importance\n",
    "        self.feature_importance = feature_importance\n",
    "        \n",
    "        return feature_importance\n",
    "    \n",
    "    def save_model(self, model, filename, preprocessor=None):\n",
    "        \"\"\"\n",
    "        Save the model to a file.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        model : object\n",
    "            Model to save\n",
    "        filename : str\n",
    "            Filename to save to\n",
    "        preprocessor : object\n",
    "            Preprocessor for the data\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        None\n",
    "        \"\"\"\n",
    "        print(f\"\\nSaving model to {filename}...\")\n",
    "        \n",
    "        # Create a dictionary with the model and preprocessor\n",
    "        model_data = {\n",
    "            'model': model,\n",
    "            'preprocessor': preprocessor,\n",
    "            'feature_importance': self.feature_importance if hasattr(self, 'feature_importance') else None,\n",
    "            'metadata': {\n",
    "                'creation_date': time.strftime('%Y-%m-%d %H:%M:%S'),\n",
    "                'random_state': self.random_state\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        # Save to file\n",
    "        joblib.dump(model_data, filename)\n",
    "        \n",
    "        print(f\"Model saved successfully to {filename}\")\n",
    "    \n",
    "    def run_pipeline(self):\n",
    "        \"\"\"\n",
    "        Run the complete machine learning pipeline.\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        results : dict\n",
    "            Dictionary with pipeline results\n",
    "        \"\"\"\n",
    "        print(\"\\nRunning complete machine learning pipeline...\")\n",
    "        \n",
    "        # 1. Load data\n",
    "        X, y = self.load_data()\n",
    "        \n",
    "        # 2. Split data\n",
    "        X_train, X_val, X_test, y_train, y_val, y_test = self.split_data(X, y)\n",
    "        \n",
    "        # 3. Exploratory Data Analysis\n",
    "        eda_results = self.exploratory_data_analysis(X_train, y_train)\n",
    "        \n",
    "        # 4. Feature Engineering\n",
    "        X_train_fe, X_val_fe, X_test_fe = self.feature_engineering(X_train, X_val, X_test)\n",
    "        \n",
    "        # 5. Feature Selection\n",
    "        feature_selection_results = self.feature_selection(X_train_fe, y_train, X_val_fe, y_val)\n",
    "        \n",
    "        # 6. Class Imbalance Handling\n",
    "        imbalance_results = self.handle_class_imbalance(X_train_fe, y_train)\n",
    "        \n",
    "        # 7. Train multiple models\n",
    "        model_results = self.train_models(X_train_fe, y_train, X_val_fe, y_val)\n",
    "        \n",
    "        # 8. Hyperparameter tuning for the best model\n",
    "        best_model_name = self.best_model_info['model_name']\n",
    "        best_technique = self.best_model_info['technique']\n",
    "        \n",
    "        best_params, best_model = self.hyperparameter_tuning(\n",
    "            X_train_fe, y_train, X_val_fe, y_val,\n",
    "            best_model_name, best_technique\n",
    "        )\n",
    "        \n",
    "        # 9. Create ensemble\n",
    "        ensemble_results = self.create_ensemble(X_train_fe, y_train, X_val_fe, y_val)\n",
    "        \n",
    "        # 10. Evaluate the best model\n",
    "        best_tuned_model = self.best_tuned_model['model']\n",
    "        best_preprocessor = self.best_tuned_model['preprocessor']\n",
    "        \n",
    "        best_metrics = self.evaluate_model(\n",
    "            best_tuned_model, X_test_fe, y_test,\n",
    "            preprocessor=best_preprocessor\n",
    "        )\n",
    "        \n",
    "        # 11. Evaluate the best ensemble\n",
    "        best_ensemble_name, best_ensemble_info = self.best_ensemble\n",
    "        \n",
    "        if best_ensemble_name == 'weighted':\n",
    "            best_ensemble_model = best_ensemble_info['ensemble']\n",
    "            ensemble_metrics = self.evaluate_model(\n",
    "                best_ensemble_model, X_test_fe, y_test,\n",
    "                is_ensemble=True, ensemble_type='weighted'\n",
    "            )\n",
    "        else:\n",
    "            best_ensemble_model = best_ensemble_info['ensemble']\n",
    "            best_ensemble_preprocessor = best_ensemble_info['preprocessor']\n",
    "            \n",
    "            ensemble_metrics = self.evaluate_model(\n",
    "                best_ensemble_model, X_test_fe, y_test,\n",
    "                preprocessor=best_ensemble_preprocessor,\n",
    "                is_ensemble=True, ensemble_type=best_ensemble_name\n",
    "            )\n",
    "        \n",
    "        # 12. Interpret the best model\n",
    "        feature_importance = self.interpret_model(\n",
    "            best_tuned_model, X_test_fe,\n",
    "            preprocessor=best_preprocessor\n",
    "        )\n",
    "        \n",
    "        # 13. Save the best model\n",
    "        if best_metrics['auc_roc'] > ensemble_metrics['auc_roc']:\n",
    "            print(\"\\nBest tuned model outperforms ensemble. Selecting as final model.\")\n",
    "            final_model = best_tuned_model\n",
    "            final_preprocessor = best_preprocessor\n",
    "            final_metrics = best_metrics\n",
    "        else:\n",
    "            print(\"\\nEnsemble outperforms best tuned model. Selecting as final model.\")\n",
    "            final_model = best_ensemble_model\n",
    "            final_preprocessor = best_ensemble_info.get('preprocessor', None)\n",
    "            final_metrics = ensemble_metrics\n",
    "        \n",
    "        # Save the final model\n",
    "        self.save_model(final_model, 'adult_income_model.joblib', final_preprocessor)\n",
    "        \n",
    "        # Compile results\n",
    "        results = {\n",
    "            'eda_results': eda_results,\n",
    "            'feature_selection_results': feature_selection_results,\n",
    "            'imbalance_results': imbalance_results,\n",
    "            'model_results': model_results,\n",
    "            'best_model': {\n",
    "                'name': best_model_name,\n",
    "                'technique': best_technique,\n",
    "                'params': best_params,\n",
    "                'metrics': best_metrics\n",
    "            },\n",
    "            'ensemble_results': ensemble_results,\n",
    "            'final_model': {\n",
    "                'model': final_model,\n",
    "                'preprocessor': final_preprocessor,\n",
    "                'metrics': final_metrics,\n",
    "                'feature_importance': feature_importance\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        # Print final results\n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        print(\"PIPELINE COMPLETED SUCCESSFULLY\")\n",
    "        print(\"=\"*50)\n",
    "        \n",
    "        print(f\"\\nFinal model AUC-ROC: {final_metrics['auc_roc']:.4f}\")\n",
    "        print(f\"Final model accuracy: {final_metrics['accuracy']:.4f}\")\n",
    "        print(f\"Final model F1 score: {final_metrics['f1']:.4f}\")\n",
    "        \n",
    "        print(\"\\nTop 10 important features:\")\n",
    "        print(feature_importance.head(10))\n",
    "        \n",
    "        return results\n",
    "\n",
    "# Execute the pipeline if run as a script\n",
    "if __name__ == \"__main__\":\n",
    "    # Create the classifier\n",
    "    classifier = AdultIncomeClassifier()\n",
    "    \n",
    "    # Run the pipeline\n",
    "    results = classifier.run_pipeline()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ssd-KmRqJrsb-py3.13",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
