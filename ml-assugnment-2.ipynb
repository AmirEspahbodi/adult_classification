{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":472,"sourceType":"datasetVersion","datasetId":222}],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"%pip install skfeature-chappers","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-15T11:45:54.379897Z","iopub.execute_input":"2025-03-15T11:45:54.380132Z","iopub.status.idle":"2025-03-15T11:45:59.106617Z","shell.execute_reply.started":"2025-03-15T11:45:54.380111Z","shell.execute_reply":"2025-03-15T11:45:59.105729Z"}},"outputs":[{"name":"stdout","text":"Collecting skfeature-chappers\n  Downloading skfeature_chappers-1.1.0-py3-none-any.whl.metadata (926 bytes)\nRequirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from skfeature-chappers) (1.2.2)\nRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from skfeature-chappers) (2.2.3)\nRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from skfeature-chappers) (1.26.4)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy->skfeature-chappers) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy->skfeature-chappers) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy->skfeature-chappers) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy->skfeature-chappers) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy->skfeature-chappers) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy->skfeature-chappers) (2.4.1)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->skfeature-chappers) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->skfeature-chappers) (2025.1)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->skfeature-chappers) (2025.1)\nRequirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->skfeature-chappers) (1.13.1)\nRequirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->skfeature-chappers) (1.4.2)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->skfeature-chappers) (3.5.0)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->skfeature-chappers) (1.17.0)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy->skfeature-chappers) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy->skfeature-chappers) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy->skfeature-chappers) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy->skfeature-chappers) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy->skfeature-chappers) (2024.2.0)\nDownloading skfeature_chappers-1.1.0-py3-none-any.whl (66 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.3/66.3 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: skfeature-chappers\nSuccessfully installed skfeature-chappers-1.1.0\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom scipy import stats\nfrom collections import Counter\n\n# Data processing libraries\nfrom sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom imblearn.over_sampling import SMOTE, ADASYN\nfrom imblearn.under_sampling import RandomUnderSampler\n\n# Feature selection and extraction\nfrom sklearn.feature_selection import SelectKBest, f_classif, mutual_info_classif, RFE\nfrom sklearn.decomposition import PCA\nfrom skfeature.function.information_theoretical_based import MRMR\n\n# ML models\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, VotingClassifier, AdaBoostClassifier, StackingClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.neural_network import MLPClassifier\nfrom xgboost import XGBClassifier\nfrom lightgbm import LGBMClassifier\nfrom sklearn.impute import SimpleImputer\n\n# Metrics\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix, classification_report\n\n# For reproducibility\nnp.random.seed(42)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-15T11:45:59.107475Z","iopub.execute_input":"2025-03-15T11:45:59.107697Z","iopub.status.idle":"2025-03-15T11:46:05.006406Z","shell.execute_reply.started":"2025-03-15T11:45:59.107677Z","shell.execute_reply":"2025-03-15T11:46:05.005722Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"def generate_synthetic_data(sample_data, n_samples):\n    \"\"\"\n    Generate synthetic data based on the sample to demonstrate the pipeline\n    \"\"\"\n    # Create a larger synthetic dataset based on the patterns in the samples\n    synth_data = pd.DataFrame(columns=sample_data.columns)\n    \n    # Get the numerical and categorical columns\n    numerical_cols = ['age', 'fnlwgt', 'education_num', 'capital_gain', 'capital_loss', 'hours_per_week']\n    categorical_cols = [col for col in sample_data.columns if col not in numerical_cols and col != 'income']\n    \n    # Calculate statistics for numerical columns\n    num_stats = {col: (sample_data[col].mean(), sample_data[col].std()) for col in numerical_cols}\n    \n    # Get unique values for categorical columns\n    cat_values = {col: sample_data[col].unique() for col in categorical_cols}\n    \n    # Generate synthetic samples\n    synthetic_samples = []\n    \n    # Ensure class balance is closer to 75-25 (<=50K vs >50K)\n    for i in range(n_samples):\n        new_sample = {}\n        \n        # Generate numerical features with realistic distributions\n        for col in numerical_cols:\n            mean, std = num_stats[col]\n            new_sample[col] = max(0, int(np.random.normal(mean, std)))\n        \n        # Generate categorical features based on sample distribution\n        for col in categorical_cols:\n            new_sample[col] = np.random.choice(cat_values[col])\n        \n        # Assign label class - 75% <=50K, 25% >50K for demonstration\n        if i < 0.75 * n_samples:\n            new_sample['label'] = '<=50K'\n        else:\n            new_sample['label'] = '>50K'\n            \n            # Higher education, age, hours worked for >50K group\n            new_sample['education_num'] = max(12, new_sample['education_num'])\n            new_sample['age'] = max(30, new_sample['age'])\n            new_sample['hours_per_week'] = max(35, new_sample['hours_per_week'])\n            \n            # Higher capital gain probability\n            if np.random.random() > 0.7:\n                new_sample['capital-gain'] = np.random.randint(1000, 10000)\n        \n        synthetic_samples.append(new_sample)\n    \n    return pd.DataFrame(synthetic_samples)\n\n\n# Load the dataset\ndef load_data():\n    \"\"\"\n    Load the Adult dataset either from a URL or use a sample if URL not provided\n    \"\"\"\n    # For this implementation, we'll use the sample data provided\n    # In a real scenario, you would download from the URL\n    \n\n    # Define column names based on dataset description.\n    columns = [\"age\", \"workclass\", \"fnlwgt\", \"education\", \"education_num\", \n               \"marital_status\", \"occupation\", \"relationship\", \"race\", \"sex\", \n               \"capital_gain\", \"capital_loss\", \"hours_per_week\", \"native_country\", \"label\"]\n    \n    # Load dataset (treat \" ?\" as NA).\n    data = pd.read_csv(\"https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data\", \n                       header=None, names=columns, na_values=\" ?\")\n    \n    data.dropna(inplace=True)\n    \n    # Display the first few rows of the DataFrame\n    print(data.head())\n    \n    # Correctly assign the column names\n    data.columns = columns\n    \n    # Display the first few rows to verify the changes\n    print(data.head())\n    \n    # Note: In an actual implementation, you'd use:\n    # data = pd.read_csv(file_url, names=columns)\n    \n    # For demonstration, let's create a synthetic expanded dataset based on the sample\n    # This will allow us to properly demonstrate all required techniques\n    \n    expanded_data = generate_synthetic_data(data, 1000)  \n    \n    return expanded_data\n\nload_data()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-15T11:46:05.007317Z","iopub.execute_input":"2025-03-15T11:46:05.008017Z","iopub.status.idle":"2025-03-15T11:46:05.584376Z","shell.execute_reply.started":"2025-03-15T11:46:05.007982Z","shell.execute_reply":"2025-03-15T11:46:05.583478Z"}},"outputs":[{"name":"stdout","text":"   age          workclass  fnlwgt   education  education_num  \\\n0   39          State-gov   77516   Bachelors             13   \n1   50   Self-emp-not-inc   83311   Bachelors             13   \n2   38            Private  215646     HS-grad              9   \n3   53            Private  234721        11th              7   \n4   28            Private  338409   Bachelors             13   \n\n        marital_status          occupation    relationship    race      sex  \\\n0        Never-married        Adm-clerical   Not-in-family   White     Male   \n1   Married-civ-spouse     Exec-managerial         Husband   White     Male   \n2             Divorced   Handlers-cleaners   Not-in-family   White     Male   \n3   Married-civ-spouse   Handlers-cleaners         Husband   Black     Male   \n4   Married-civ-spouse      Prof-specialty            Wife   Black   Female   \n\n   capital_gain  capital_loss  hours_per_week  native_country   label  \n0          2174             0              40   United-States   <=50K  \n1             0             0              13   United-States   <=50K  \n2             0             0              40   United-States   <=50K  \n3             0             0              40   United-States   <=50K  \n4             0             0              40            Cuba   <=50K  \n   age          workclass  fnlwgt   education  education_num  \\\n0   39          State-gov   77516   Bachelors             13   \n1   50   Self-emp-not-inc   83311   Bachelors             13   \n2   38            Private  215646     HS-grad              9   \n3   53            Private  234721        11th              7   \n4   28            Private  338409   Bachelors             13   \n\n        marital_status          occupation    relationship    race      sex  \\\n0        Never-married        Adm-clerical   Not-in-family   White     Male   \n1   Married-civ-spouse     Exec-managerial         Husband   White     Male   \n2             Divorced   Handlers-cleaners   Not-in-family   White     Male   \n3   Married-civ-spouse   Handlers-cleaners         Husband   Black     Male   \n4   Married-civ-spouse      Prof-specialty            Wife   Black   Female   \n\n   capital_gain  capital_loss  hours_per_week  native_country   label  \n0          2174             0              40   United-States   <=50K  \n1             0             0              13   United-States   <=50K  \n2             0             0              40   United-States   <=50K  \n3             0             0              40   United-States   <=50K  \n4             0             0              40            Cuba   <=50K  \n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/pandas/io/formats/format.py:1458: RuntimeWarning: invalid value encountered in greater\n  has_large_values = (abs_vals > 1e6).any()\n/usr/local/lib/python3.10/dist-packages/pandas/io/formats/format.py:1459: RuntimeWarning: invalid value encountered in less\n  has_small_values = ((abs_vals < 10 ** (-self.digits)) & (abs_vals > 0)).any()\n/usr/local/lib/python3.10/dist-packages/pandas/io/formats/format.py:1459: RuntimeWarning: invalid value encountered in greater\n  has_small_values = ((abs_vals < 10 ** (-self.digits)) & (abs_vals > 0)).any()\n","output_type":"stream"},{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"     age  fnlwgt  education_num  capital_gain  capital_loss  hours_per_week  \\\n0     44  175185             11         12372             0              38   \n1     38  144596              8           221           178              31   \n2     34  186641             10          6014            31              40   \n3     22  146576              8         12381           218              24   \n4     26  315375             12         20763           194              60   \n..   ...     ...            ...           ...           ...             ...   \n995   53  276720             12             0            28              35   \n996   64  120436             12             0             0              35   \n997   30  103233             12             7           175              56   \n998   43  104070             12             0             0              46   \n999   42  161698             12         11191             0              48   \n\n             workclass     education          marital_status  \\\n0              Private   Prof-school               Separated   \n1          Federal-gov    Assoc-acdm   Married-spouse-absent   \n2     Self-emp-not-inc       Masters   Married-spouse-absent   \n3            Local-gov          12th      Married-civ-spouse   \n4     Self-emp-not-inc     Doctorate   Married-spouse-absent   \n..                 ...           ...                     ...   \n995          State-gov    Assoc-acdm           Never-married   \n996       Self-emp-inc       7th-8th                 Widowed   \n997   Self-emp-not-inc       1st-4th           Never-married   \n998   Self-emp-not-inc           9th               Separated   \n999        Without-pay     Doctorate      Married-civ-spouse   \n\n             occupation relationship    race      sex       native_country  \\\n0        Prof-specialty         Wife   Other   Female               France   \n1     Machine-op-inspct         Wife   Other     Male             Honduras   \n2       Protective-serv    Own-child   Other   Female               Greece   \n3          Tech-support    Own-child   Black   Female            Nicaragua   \n4       Protective-serv    Own-child   Other     Male                Japan   \n..                  ...          ...     ...      ...                  ...   \n995        Armed-Forces      Husband   White   Female   Holand-Netherlands   \n996        Armed-Forces         Wife   Black     Male        United-States   \n997      Prof-specialty      Husband   Black     Male                China   \n998     Protective-serv      Husband   Black   Female                India   \n999        Adm-clerical      Husband   White     Male          El-Salvador   \n\n     label  capital-gain  \n0    <=50K           NaN  \n1    <=50K           NaN  \n2    <=50K           NaN  \n3    <=50K           NaN  \n4    <=50K           NaN  \n..     ...           ...  \n995   >50K           NaN  \n996   >50K        6431.0  \n997   >50K        4742.0  \n998   >50K           NaN  \n999   >50K        4376.0  \n\n[1000 rows x 16 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>age</th>\n      <th>fnlwgt</th>\n      <th>education_num</th>\n      <th>capital_gain</th>\n      <th>capital_loss</th>\n      <th>hours_per_week</th>\n      <th>workclass</th>\n      <th>education</th>\n      <th>marital_status</th>\n      <th>occupation</th>\n      <th>relationship</th>\n      <th>race</th>\n      <th>sex</th>\n      <th>native_country</th>\n      <th>label</th>\n      <th>capital-gain</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>44</td>\n      <td>175185</td>\n      <td>11</td>\n      <td>12372</td>\n      <td>0</td>\n      <td>38</td>\n      <td>Private</td>\n      <td>Prof-school</td>\n      <td>Separated</td>\n      <td>Prof-specialty</td>\n      <td>Wife</td>\n      <td>Other</td>\n      <td>Female</td>\n      <td>France</td>\n      <td>&lt;=50K</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>38</td>\n      <td>144596</td>\n      <td>8</td>\n      <td>221</td>\n      <td>178</td>\n      <td>31</td>\n      <td>Federal-gov</td>\n      <td>Assoc-acdm</td>\n      <td>Married-spouse-absent</td>\n      <td>Machine-op-inspct</td>\n      <td>Wife</td>\n      <td>Other</td>\n      <td>Male</td>\n      <td>Honduras</td>\n      <td>&lt;=50K</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>34</td>\n      <td>186641</td>\n      <td>10</td>\n      <td>6014</td>\n      <td>31</td>\n      <td>40</td>\n      <td>Self-emp-not-inc</td>\n      <td>Masters</td>\n      <td>Married-spouse-absent</td>\n      <td>Protective-serv</td>\n      <td>Own-child</td>\n      <td>Other</td>\n      <td>Female</td>\n      <td>Greece</td>\n      <td>&lt;=50K</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>22</td>\n      <td>146576</td>\n      <td>8</td>\n      <td>12381</td>\n      <td>218</td>\n      <td>24</td>\n      <td>Local-gov</td>\n      <td>12th</td>\n      <td>Married-civ-spouse</td>\n      <td>Tech-support</td>\n      <td>Own-child</td>\n      <td>Black</td>\n      <td>Female</td>\n      <td>Nicaragua</td>\n      <td>&lt;=50K</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>26</td>\n      <td>315375</td>\n      <td>12</td>\n      <td>20763</td>\n      <td>194</td>\n      <td>60</td>\n      <td>Self-emp-not-inc</td>\n      <td>Doctorate</td>\n      <td>Married-spouse-absent</td>\n      <td>Protective-serv</td>\n      <td>Own-child</td>\n      <td>Other</td>\n      <td>Male</td>\n      <td>Japan</td>\n      <td>&lt;=50K</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>995</th>\n      <td>53</td>\n      <td>276720</td>\n      <td>12</td>\n      <td>0</td>\n      <td>28</td>\n      <td>35</td>\n      <td>State-gov</td>\n      <td>Assoc-acdm</td>\n      <td>Never-married</td>\n      <td>Armed-Forces</td>\n      <td>Husband</td>\n      <td>White</td>\n      <td>Female</td>\n      <td>Holand-Netherlands</td>\n      <td>&gt;50K</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>996</th>\n      <td>64</td>\n      <td>120436</td>\n      <td>12</td>\n      <td>0</td>\n      <td>0</td>\n      <td>35</td>\n      <td>Self-emp-inc</td>\n      <td>7th-8th</td>\n      <td>Widowed</td>\n      <td>Armed-Forces</td>\n      <td>Wife</td>\n      <td>Black</td>\n      <td>Male</td>\n      <td>United-States</td>\n      <td>&gt;50K</td>\n      <td>6431.0</td>\n    </tr>\n    <tr>\n      <th>997</th>\n      <td>30</td>\n      <td>103233</td>\n      <td>12</td>\n      <td>7</td>\n      <td>175</td>\n      <td>56</td>\n      <td>Self-emp-not-inc</td>\n      <td>1st-4th</td>\n      <td>Never-married</td>\n      <td>Prof-specialty</td>\n      <td>Husband</td>\n      <td>Black</td>\n      <td>Male</td>\n      <td>China</td>\n      <td>&gt;50K</td>\n      <td>4742.0</td>\n    </tr>\n    <tr>\n      <th>998</th>\n      <td>43</td>\n      <td>104070</td>\n      <td>12</td>\n      <td>0</td>\n      <td>0</td>\n      <td>46</td>\n      <td>Self-emp-not-inc</td>\n      <td>9th</td>\n      <td>Separated</td>\n      <td>Protective-serv</td>\n      <td>Husband</td>\n      <td>Black</td>\n      <td>Female</td>\n      <td>India</td>\n      <td>&gt;50K</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>999</th>\n      <td>42</td>\n      <td>161698</td>\n      <td>12</td>\n      <td>11191</td>\n      <td>0</td>\n      <td>48</td>\n      <td>Without-pay</td>\n      <td>Doctorate</td>\n      <td>Married-civ-spouse</td>\n      <td>Adm-clerical</td>\n      <td>Husband</td>\n      <td>White</td>\n      <td>Male</td>\n      <td>El-Salvador</td>\n      <td>&gt;50K</td>\n      <td>4376.0</td>\n    </tr>\n  </tbody>\n</table>\n<p>1000 rows × 16 columns</p>\n</div>"},"metadata":{}}],"execution_count":3},{"cell_type":"code","source":"# Data splitting function \ndef split_data(data, test_size=0.2, val_size=0.1):\n    \"\"\"\n    Split data into train (70%), validation (10%), and test (20%) sets\n    \"\"\"\n    # First split: 80% train+val, 20% test\n    X = data.drop('label', axis=1)\n    y = data['label']\n    \n    X_train_val, X_test, y_train_val, y_test = train_test_split(\n        X, y, test_size=test_size, random_state=42, stratify=y)\n    \n    # Second split: 70% train, 10% validation (87.5% of train_val is train)\n    val_ratio = val_size / (1 - test_size)\n    X_train, X_val, y_train, y_val = train_test_split(\n        X_train_val, y_train_val, test_size=val_ratio, random_state=42, stratify=y_train_val)\n    \n    print(f\"Training set: {X_train.shape[0]} samples\")\n    print(f\"Validation set: {X_val.shape[0]} samples\")\n    print(f\"Test set: {X_test.shape[0]} samples\")\n    \n    return X_train, X_val, X_test, y_train, y_val, y_test\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-15T11:46:05.586020Z","iopub.execute_input":"2025-03-15T11:46:05.586254Z","iopub.status.idle":"2025-03-15T11:46:05.591444Z","shell.execute_reply.started":"2025-03-15T11:46:05.586235Z","shell.execute_reply":"2025-03-15T11:46:05.590662Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"# Data preprocessing function\ndef preprocess_data(X_train, X_val, X_test):\n    \"\"\"\n    Preprocess the data: handle categorical features, missing values, scaling\n    \"\"\"\n    # Identify numerical and categorical columns\n    numerical_cols = X_train.select_dtypes(include=['int64', 'float64']).columns.tolist()\n    categorical_cols = X_train.select_dtypes(include=['object']).columns.tolist()\n    \n    # Create preprocessing pipelines\n    numerical_transformer = Pipeline(steps=[\n        ('imputer', SimpleImputer(strategy='median')),\n        ('scaler', StandardScaler())\n    ])\n    \n    categorical_transformer = Pipeline(steps=[\n        ('imputer', SimpleImputer(strategy='most_frequent')),\n        ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))\n    ])\n    \n    # Combine transformers\n    preprocessor = ColumnTransformer(\n        transformers=[\n            ('num', numerical_transformer, numerical_cols),\n            ('cat', categorical_transformer, categorical_cols)\n        ])\n    \n    # Fit and transform the data\n    X_train_processed = preprocessor.fit_transform(X_train)\n    X_val_processed = preprocessor.transform(X_val)\n    X_test_processed = preprocessor.transform(X_test)\n    \n    # Get feature names after one-hot encoding\n    cat_feature_names = []\n    if categorical_cols:\n        encoder = preprocessor.named_transformers_['cat'].named_steps['onehot']\n        cat_feature_names = encoder.get_feature_names_out(categorical_cols)\n    \n    feature_names = np.array(numerical_cols + list(cat_feature_names))\n    \n    return X_train_processed, X_val_processed, X_test_processed, feature_names, preprocessor","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-15T11:46:05.592600Z","iopub.execute_input":"2025-03-15T11:46:05.592854Z","iopub.status.idle":"2025-03-15T11:46:05.613780Z","shell.execute_reply.started":"2025-03-15T11:46:05.592833Z","shell.execute_reply":"2025-03-15T11:46:05.613012Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"# Part 1: Feature Selection Functions\ndef mrmr_feature_selection(X, y, k=10):\n    \"\"\"\n    Minimum Redundancy Maximum Relevance feature selection\n    \"\"\"\n    try:\n        # Convert to numpy arrays if not already\n        X_np = X if isinstance(X, np.ndarray) else X.to_numpy()\n        y_np = y if isinstance(y, np.ndarray) else y.to_numpy()\n        \n        # Label encode the target if it's categorical\n        if y_np.dtype == object or y_np.dtype.kind in ['U', 'S']:\n            le = LabelEncoder()\n            y_np = le.fit_transform(y_np)\n        \n        # Run MRMR\n        selected_features = MRMR.mrmr(X_np, y_np, n_selected_features=k)\n        \n        return selected_features\n    except Exception as e:\n        print(f\"MRMR feature selection failed: {e}\")\n        return None\n\ndef select_k_best_features(X, y, k=10, score_func=f_classif):\n    \"\"\"\n    Select K best features based on statistical tests\n    \"\"\"\n    selector = SelectKBest(score_func=score_func, k=k)\n    X_new = selector.fit_transform(X, y)\n    \n    # Get selected feature indices\n    selected_indices = selector.get_support(indices=True)\n    \n    return selected_indices, X_new\n\ndef recursive_feature_elimination(X, y, n_features=10):\n    \"\"\"\n    Recursive Feature Elimination using Random Forest\n    \"\"\"\n    estimator = RandomForestClassifier(n_estimators=100, random_state=42)\n    selector = RFE(estimator, n_features_to_select=n_features, step=1)\n    selector = selector.fit(X, y)\n    \n    # Get selected feature indices\n    selected_indices = np.where(selector.support_)[0]\n    \n    return selected_indices, selector.transform(X)\n\n# Part 1: Feature Extraction Function\ndef apply_pca(X_train, X_val, X_test, n_components=None, variance_threshold=0.95):\n    \"\"\"\n    Apply PCA for dimensionality reduction\n    If n_components is None, select components to explain variance_threshold of variance\n    \"\"\"\n    if n_components is None:\n        # Start with all components\n        pca_full = PCA(random_state=42)\n        pca_full.fit(X_train)\n        \n        # Calculate cumulative explained variance\n        cumulative_variance = np.cumsum(pca_full.explained_variance_ratio_)\n        \n        # Find number of components needed to explain variance_threshold\n        n_components = np.argmax(cumulative_variance >= variance_threshold) + 1\n        print(f\"Selected {n_components} components to explain {variance_threshold*100:.1f}% of variance\")\n    \n    # Apply PCA with selected number of components\n    pca = PCA(n_components=n_components, random_state=42)\n    X_train_pca = pca.fit_transform(X_train)\n    X_val_pca = pca.transform(X_val)\n    X_test_pca = pca.transform(X_test)\n    \n    return X_train_pca, X_val_pca, X_test_pca, pca\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-15T11:46:05.614539Z","iopub.execute_input":"2025-03-15T11:46:05.614782Z","iopub.status.idle":"2025-03-15T11:46:05.633976Z","shell.execute_reply.started":"2025-03-15T11:46:05.614762Z","shell.execute_reply":"2025-03-15T11:46:05.633163Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"# Part 2: Data Balancing Function\ndef balance_dataset(X_train, y_train, method='smote'):\n    \"\"\"\n    Balance the dataset using different techniques\n    \"\"\"\n    print(\"Class distribution before balancing:\")\n    print(pd.Series(y_train).value_counts())\n    \n    if method == 'smote':\n        sampler = SMOTE(random_state=42)\n    elif method == 'adasyn':\n        sampler = ADASYN(random_state=42)\n    elif method == 'under':\n        sampler = RandomUnderSampler(random_state=42)\n    elif method == 'hybrid':\n        # Hybrid approach: undersample majority + oversample minority\n        # First undersample the majority class\n        rus = RandomUnderSampler(sampling_strategy=0.5, random_state=42)\n        X_temp, y_temp = rus.fit_resample(X_train, y_train)\n        \n        # Then oversample the minority class to achieve perfect balance\n        sampler = SMOTE(random_state=42)\n        X_balanced, y_balanced = sampler.fit_resample(X_temp, y_temp)\n        \n        print(\"Class distribution after hybrid balancing:\")\n        print(pd.Series(y_balanced).value_counts())\n        \n        return X_balanced, y_balanced\n    else:\n        print(\"No balancing applied\")\n        return X_train, y_train\n    \n    # Apply the selected sampling method\n    X_balanced, y_balanced = sampler.fit_resample(X_train, y_train)\n    \n    print(\"Class distribution after balancing:\")\n    print(pd.Series(y_balanced).value_counts())\n    \n    return X_balanced, y_balanced\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-15T11:46:05.634795Z","iopub.execute_input":"2025-03-15T11:46:05.635093Z","iopub.status.idle":"2025-03-15T11:46:05.652711Z","shell.execute_reply.started":"2025-03-15T11:46:05.635064Z","shell.execute_reply":"2025-03-15T11:46:05.651838Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"# Part 3: Data Augmentation Function\ndef augment_data(X, y, noise_level=0.05, n_samples=None):\n    \"\"\"\n    Augment data by adding Gaussian noise\n    \"\"\"\n    if n_samples is None:\n        n_samples = int(X.shape[0] * 0.3)  # Add 30% more samples by default\n    \n    # Create copies of existing samples with added noise\n    augmented_X = []\n    augmented_y = []\n    \n    # Get class distribution\n    class_counts = Counter(y)\n    minority_class = min(class_counts, key=class_counts.get)\n    \n    # Focus augmentation on minority class for better balance\n    indices = np.where(y == minority_class)[0]\n    if len(indices) == 0:  # fallback if labels are not properly encoded\n        indices = np.random.choice(len(y), size=n_samples, replace=True)\n    else:\n        # Sample with replacement if we need more than available minority samples\n        indices = np.random.choice(indices, size=n_samples, replace=len(indices) < n_samples)\n    \n    for idx in indices:\n        # Get the original sample\n        sample = X[idx].copy()\n        \n        # Add Gaussian noise to each feature\n        noise = np.random.normal(0, noise_level, size=sample.shape)\n        augmented_sample = sample + noise * np.abs(sample)  # Scale noise by feature magnitude\n        \n        augmented_X.append(augmented_sample)\n        augmented_y.append(y[idx])\n    \n    # Combine original and augmented data\n    X_augmented = np.vstack([X, np.array(augmented_X)])\n    y_augmented = np.concatenate([y, np.array(augmented_y)])\n    \n    print(f\"Data augmented from {X.shape[0]} to {X_augmented.shape[0]} samples\")\n    \n    return X_augmented, y_augmented\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-15T11:46:05.653396Z","iopub.execute_input":"2025-03-15T11:46:05.653609Z","iopub.status.idle":"2025-03-15T11:46:05.666073Z","shell.execute_reply.started":"2025-03-15T11:46:05.653591Z","shell.execute_reply":"2025-03-15T11:46:05.665260Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"# -------------------------------\n# Stacking Ensemble Functions\n# -------------------------------\ndef create_base_models():\n    \"\"\"\n    Create a dictionary of diverse base models.\n    \"\"\"\n    models = {\n        'rf': RandomForestClassifier(n_estimators=100, random_state=42),\n        'gbm': GradientBoostingClassifier(n_estimators=100, random_state=42),\n        'svm': SVC(probability=True, random_state=42),\n        'lgbm': LGBMClassifier(random_state=42),\n        'xgb': XGBClassifier(random_state=42, use_label_encoder=False, eval_metric='logloss'),\n        'mlp': MLPClassifier(max_iter=1000, random_state=42)\n    }\n    return models\n\ndef build_stacking_ensemble(X_train, y_train, X_val, y_val, voting_type='soft'):\n    \"\"\"\n    Build an ensemble using stacking or voting. (Only 'soft' and 'stacking' support predict_proba.)\n    \"\"\"\n    base_models = create_base_models()\n    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n    meta_learner = LogisticRegression(random_state=42)\n    \n    if voting_type == 'stacking':\n        ensemble = StackingClassifier(\n            estimators=[(name, model) for name, model in base_models.items()],\n            final_estimator=meta_learner,\n            cv=cv,\n            stack_method='predict_proba'\n        )\n    else:\n        ensemble = VotingClassifier(\n            estimators=[(name, model) for name, model in base_models.items()],\n            voting=voting_type  # Expecting 'soft' for probability estimates\n        )\n    ensemble.fit(X_train, y_train)\n    y_val_pred = ensemble.predict(X_val)\n    f1 = f1_score(y_val, y_val_pred, pos_label='>50K' if isinstance(y_val.iloc[0], str) else 1)\n    print(f\"{voting_type.capitalize()} Voting Ensemble Validation F1 Score: {f1:.4f}\")\n    return ensemble, f1","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-15T11:46:05.666880Z","iopub.execute_input":"2025-03-15T11:46:05.667174Z","iopub.status.idle":"2025-03-15T11:46:05.685146Z","shell.execute_reply.started":"2025-03-15T11:46:05.667142Z","shell.execute_reply":"2025-03-15T11:46:05.684519Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"# -------------------------------\n# Main Pipeline Function\n# -------------------------------\ndef run_ml_pipeline():\n    \"\"\"\n    Run the complete ML pipeline and evaluate using AUROC.\n    \"\"\"\n    print(\"Loading data...\")\n    data = load_data()\n    \n    print(\"\\nSplitting data into train, validation, and test sets...\")\n    X_train, X_val, X_test, y_train, y_val, y_test = split_data(data)\n    \n    print(\"\\nPreprocessing data...\")\n    X_train_processed, X_val_processed, X_test_processed, feature_names, preprocessor = preprocess_data(X_train, X_val, X_test)\n    \n    print(\"\\nPerforming feature selection...\")\n    k = min(15, X_train_processed.shape[1])\n    print(\"Running SelectKBest...\")\n    selected_indices_kb, X_train_kb = select_k_best_features(X_train_processed, y_train, k=k)\n    X_val_kb = X_val_processed[:, selected_indices_kb]\n    X_test_kb = X_test_processed[:, selected_indices_kb]\n    \n    print(\"Running Recursive Feature Elimination...\")\n    selected_indices_rfe, X_train_rfe = recursive_feature_elimination(X_train_processed, y_train, n_features=k)\n    X_val_rfe = X_val_processed[:, selected_indices_rfe]\n    X_test_rfe = X_test_processed[:, selected_indices_rfe]\n    \n    try:\n        print(\"Running MRMR...\")\n        selected_indices_mrmr = mrmr_feature_selection(X_train_processed, y_train, k=k)\n        if selected_indices_mrmr is not None:\n            X_train_mrmr = X_train_processed[:, selected_indices_mrmr]\n            X_val_mrmr = X_val_processed[:, selected_indices_mrmr]\n            X_test_mrmr = X_test_processed[:, selected_indices_mrmr]\n        else:\n            raise Exception(\"MRMR returned None\")\n    except Exception as e:\n        print(f\"MRMR selection failed: {e}\")\n        print(\"Defaulting to SelectKBest results\")\n        X_train_mrmr, X_val_mrmr, X_test_mrmr = X_train_kb, X_val_kb, X_test_kb\n    \n    print(\"\\nEvaluating feature selection methods using RandomForestClassifier (AUROC)...\")\n    from sklearn.ensemble import RandomForestClassifier\n    model = RandomForestClassifier(n_estimators=100, random_state=42)\n    \n    model.fit(X_train_kb, y_train)\n    kb_metric = roc_auc_score(y_val, model.predict_proba(X_val_kb)[:,1])\n    print(f\"SelectKBest AUROC: {kb_metric:.4f}\")\n    \n    model.fit(X_train_rfe, y_train)\n    rfe_metric = roc_auc_score(y_val, model.predict_proba(X_val_rfe)[:,1])\n    print(f\"RFE AUROC: {rfe_metric:.4f}\")\n    \n    model.fit(X_train_mrmr, y_train)\n    mrmr_metric = roc_auc_score(y_val, model.predict_proba(X_val_mrmr)[:,1])\n    print(f\"MRMR AUROC: {mrmr_metric:.4f}\")\n    \n    best_metric = max(kb_metric, rfe_metric, mrmr_metric)\n    if best_metric == kb_metric:\n        print(\"SelectKBest performed best\")\n        X_train_selected, X_val_selected, X_test_selected = X_train_kb, X_val_kb, X_test_kb\n        selected_indices = selected_indices_kb\n    elif best_metric == rfe_metric:\n        print(\"RFE performed best\")\n        X_train_selected, X_val_selected, X_test_selected = X_train_rfe, X_val_rfe, X_test_rfe\n        selected_indices = selected_indices_rfe\n    else:\n        print(\"MRMR performed best\")\n        X_train_selected, X_val_selected, X_test_selected = X_train_mrmr, X_val_mrmr, X_test_mrmr\n        selected_indices = selected_indices_mrmr\n    \n    if len(feature_names) > 0:\n        selected_features = feature_names[selected_indices]\n        print(f\"Selected features: {selected_features}\")\n    \n    baseline_model = RandomForestClassifier(n_estimators=100, random_state=42)\n    baseline_model.fit(X_train_processed, y_train)\n    baseline_metric = roc_auc_score(y_val, baseline_model.predict_proba(X_val_processed)[:,1])\n    \n    if best_metric <= baseline_metric * 1.02:\n        print(\"\\nFeature selection didn't significantly improve performance.\")\n        print(\"Applying PCA feature extraction...\")\n        X_train_pca, X_val_pca, X_test_pca, pca = apply_pca(X_train_processed, X_val_processed, X_test_processed)\n        model.fit(X_train_pca, y_train)\n        pca_metric = roc_auc_score(y_val, model.predict_proba(X_val_pca)[:,1])\n        print(f\"PCA AUROC: {pca_metric:.4f}\")\n        if pca_metric > best_metric:\n            print(\"PCA performed better than feature selection\")\n            X_train_final, X_val_final, X_test_final = X_train_pca, X_val_pca, X_test_pca\n        else:\n            print(\"Feature selection performed better than PCA\")\n            X_train_final, X_val_final, X_test_final = X_train_selected, X_val_selected, X_test_selected\n    else:\n        print(\"Feature selection performed well, skipping PCA\")\n        X_train_final, X_val_final, X_test_final = X_train_selected, X_val_selected, X_test_selected\n    \n    print(\"\\nChecking class balance...\")\n    class_counts = pd.Series(y_train).value_counts()\n    if len(class_counts) > 1:\n        imbalance_ratio = class_counts.max() / class_counts.min()\n        if imbalance_ratio > 1.5:\n            print(f\"Dataset is imbalanced with ratio {imbalance_ratio:.2f}\")\n            print(\"Balancing dataset...\")\n            X_train_balanced, y_train_balanced = balance_dataset(X_train_final, y_train, method='hybrid')\n        else:\n            print(f\"Dataset is relatively balanced with ratio {imbalance_ratio:.2f}\")\n            X_train_balanced, y_train_balanced = X_train_final, y_train\n    else:\n        print(\"Warning: Only one class detected in training data\")\n        X_train_balanced, y_train_balanced = X_train_final, y_train\n    \n    print(\"\\nAugmenting training data...\")\n    X_train_augmented, y_train_augmented = augment_data(X_train_balanced, y_train_balanced)\n    \n    print(\"\\nBuilding stacking ensembles (using only 'soft' and 'stacking' for probability estimates)...\")\n    soft_ensemble, _ = build_stacking_ensemble(X_train_augmented, y_train_augmented, X_val_final, y_val, voting_type='soft')\n    stacking_ensemble, _ = build_stacking_ensemble(X_train_augmented, y_train_augmented, X_val_final, y_val, voting_type='stacking')\n    \n    print(\"\\nSelecting best ensemble model based on validation AUROC...\")\n    soft_auroc = roc_auc_score(y_val, soft_ensemble.predict_proba(X_val_final)[:,1])\n    stacking_auroc = roc_auc_score(y_val, stacking_ensemble.predict_proba(X_val_final)[:,1])\n    print(f\"Soft Voting AUROC: {soft_auroc:.4f}\")\n    print(f\"Stacking Ensemble AUROC: {stacking_auroc:.4f}\")\n    \n    if stacking_auroc >= soft_auroc:\n        print(\"Stacking classifier performed best\")\n        final_ensemble = stacking_ensemble\n    else:\n        print(\"Soft voting ensemble performed best\")\n        final_ensemble = soft_ensemble\n    \n    print(\"\\nEvaluating on test set using AUROC metric...\")\n    y_test_pred_proba = final_ensemble.predict_proba(X_test_final)\n    test_auroc = roc_auc_score(y_test, y_test_pred_proba[:,1])\n    \n    # For reference, also compute F1 score (optional)\n    y_test_pred = final_ensemble.predict(X_test_final)\n    test_f1 = f1_score(y_test, y_test_pred, pos_label='>50K' if isinstance(y_test.iloc[0], str) else 1)\n    \n    print(\"\\nFinal Test Results:\")\n    print(f\"AUROC: {test_auroc:.4f}\")\n    print(f\"F1 Score: {test_f1:.4f}\")\n    print(\"\\nClassification Report:\")\n    print(classification_report(y_test, y_test_pred))\n\n    # -------------------------------\n    # Confusion Matrix Plot Section\n    # -------------------------------\n    cm = confusion_matrix(y_test, y_test_pred)\n    plt.figure(figsize=(8, 6))\n    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", \n                xticklabels=['<=50K', '>50K'], yticklabels=['<=50K', '>50K'])\n    plt.ylabel(\"Actual\")\n    plt.xlabel(\"Predicted\")\n    plt.title(\"Confusion Matrix\")\n    plt.show()\n    \n    return {\n        'model': final_ensemble,\n        'preprocessor': preprocessor,\n        'auroc': test_auroc,\n        'f1_score': test_f1\n    }\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-15T11:46:05.685967Z","iopub.execute_input":"2025-03-15T11:46:05.686244Z","iopub.status.idle":"2025-03-15T11:46:05.705044Z","shell.execute_reply.started":"2025-03-15T11:46:05.686216Z","shell.execute_reply":"2025-03-15T11:46:05.704234Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"results = run_ml_pipeline()\nprint(\"\\nMachine Learning Pipeline Completed Successfully\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-15T11:46:05.705808Z","iopub.execute_input":"2025-03-15T11:46:05.706111Z","iopub.status.idle":"2025-03-15T11:46:49.029641Z","shell.execute_reply.started":"2025-03-15T11:46:05.706083Z","shell.execute_reply":"2025-03-15T11:46:49.028693Z"}},"outputs":[{"name":"stdout","text":"Loading data...\n   age          workclass  fnlwgt   education  education_num  \\\n0   39          State-gov   77516   Bachelors             13   \n1   50   Self-emp-not-inc   83311   Bachelors             13   \n2   38            Private  215646     HS-grad              9   \n3   53            Private  234721        11th              7   \n4   28            Private  338409   Bachelors             13   \n\n        marital_status          occupation    relationship    race      sex  \\\n0        Never-married        Adm-clerical   Not-in-family   White     Male   \n1   Married-civ-spouse     Exec-managerial         Husband   White     Male   \n2             Divorced   Handlers-cleaners   Not-in-family   White     Male   \n3   Married-civ-spouse   Handlers-cleaners         Husband   Black     Male   \n4   Married-civ-spouse      Prof-specialty            Wife   Black   Female   \n\n   capital_gain  capital_loss  hours_per_week  native_country   label  \n0          2174             0              40   United-States   <=50K  \n1             0             0              13   United-States   <=50K  \n2             0             0              40   United-States   <=50K  \n3             0             0              40   United-States   <=50K  \n4             0             0              40            Cuba   <=50K  \n   age          workclass  fnlwgt   education  education_num  \\\n0   39          State-gov   77516   Bachelors             13   \n1   50   Self-emp-not-inc   83311   Bachelors             13   \n2   38            Private  215646     HS-grad              9   \n3   53            Private  234721        11th              7   \n4   28            Private  338409   Bachelors             13   \n\n        marital_status          occupation    relationship    race      sex  \\\n0        Never-married        Adm-clerical   Not-in-family   White     Male   \n1   Married-civ-spouse     Exec-managerial         Husband   White     Male   \n2             Divorced   Handlers-cleaners   Not-in-family   White     Male   \n3   Married-civ-spouse   Handlers-cleaners         Husband   Black     Male   \n4   Married-civ-spouse      Prof-specialty            Wife   Black   Female   \n\n   capital_gain  capital_loss  hours_per_week  native_country   label  \n0          2174             0              40   United-States   <=50K  \n1             0             0              13   United-States   <=50K  \n2             0             0              40   United-States   <=50K  \n3             0             0              40   United-States   <=50K  \n4             0             0              40            Cuba   <=50K  \n\nSplitting data into train, validation, and test sets...\nTraining set: 700 samples\nValidation set: 100 samples\nTest set: 200 samples\n\nPreprocessing data...\n\nPerforming feature selection...\nRunning SelectKBest...\nRunning Recursive Feature Elimination...\nRunning MRMR...\n\nEvaluating feature selection methods using RandomForestClassifier (AUROC)...\nSelectKBest AUROC: 0.9771\nRFE AUROC: 0.9883\nMRMR AUROC: 0.9755\nRFE performed best\nSelected features: ['age' 'fnlwgt' 'education_num' 'capital_gain' 'capital_loss'\n 'hours_per_week' 'capital-gain' 'workclass_ Self-emp-not-inc'\n 'marital_status_ Never-married' 'occupation_ Armed-Forces'\n 'relationship_ Not-in-family' 'relationship_ Other-relative'\n 'race_ Amer-Indian-Eskimo' 'race_ White' 'sex_ Male']\n\nFeature selection didn't significantly improve performance.\nApplying PCA feature extraction...\nSelected 64 components to explain 95.0% of variance\nPCA AUROC: 0.8827\nFeature selection performed better than PCA\n\nChecking class balance...\nDataset is imbalanced with ratio 3.00\nBalancing dataset...\nClass distribution before balancing:\nlabel\n<=50K    525\n>50K     175\nName: count, dtype: int64\nClass distribution after hybrid balancing:\nlabel\n<=50K    350\n>50K     350\nName: count, dtype: int64\n\nAugmenting training data...\nData augmented from 700 to 910 samples\n\nBuilding stacking ensembles (using only 'soft' and 'stacking' for probability estimates)...\n[LightGBM] [Info] Number of positive: 350, number of negative: 560\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000255 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 1899\n[LightGBM] [Info] Number of data points in the train set: 910, number of used features: 15\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.384615 -> initscore=-0.470004\n[LightGBM] [Info] Start training from score -0.470004\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\nSoft Voting Ensemble Validation F1 Score: 0.8929\n[LightGBM] [Info] Number of positive: 350, number of negative: 560\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000382 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 1899\n[LightGBM] [Info] Number of data points in the train set: 910, number of used features: 15\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.384615 -> initscore=-0.470004\n[LightGBM] [Info] Start training from score -0.470004\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Info] Number of positive: 280, number of negative: 448\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000310 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 1601\n[LightGBM] [Info] Number of data points in the train set: 728, number of used features: 15\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.384615 -> initscore=-0.470004\n[LightGBM] [Info] Start training from score -0.470004\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Info] Number of positive: 280, number of negative: 448\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000227 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 1602\n[LightGBM] [Info] Number of data points in the train set: 728, number of used features: 15\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.384615 -> initscore=-0.470004\n[LightGBM] [Info] Start training from score -0.470004\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Info] Number of positive: 280, number of negative: 448\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000252 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 1591\n[LightGBM] [Info] Number of data points in the train set: 728, number of used features: 15\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.384615 -> initscore=-0.470004\n[LightGBM] [Info] Start training from score -0.470004\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Info] Number of positive: 280, number of negative: 448\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000257 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 1598\n[LightGBM] [Info] Number of data points in the train set: 728, number of used features: 15\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.384615 -> initscore=-0.470004\n[LightGBM] [Info] Start training from score -0.470004\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Info] Number of positive: 280, number of negative: 448\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000200 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 1605\n[LightGBM] [Info] Number of data points in the train set: 728, number of used features: 15\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.384615 -> initscore=-0.470004\n[LightGBM] [Info] Start training from score -0.470004\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\nStacking Voting Ensemble Validation F1 Score: 0.8519\n\nSelecting best ensemble model based on validation AUROC...\nSoft Voting AUROC: 0.9813\nStacking Ensemble AUROC: 0.9840\nStacking classifier performed best\n\nEvaluating on test set using AUROC metric...\n\nFinal Test Results:\nAUROC: 0.9827\nF1 Score: 0.8762\n\nClassification Report:\n              precision    recall  f1-score   support\n\n       <=50K       0.97      0.94      0.96       150\n        >50K       0.84      0.92      0.88        50\n\n    accuracy                           0.94       200\n   macro avg       0.90      0.93      0.92       200\nweighted avg       0.94      0.94      0.94       200\n\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<Figure size 800x600 with 2 Axes>","image/png":"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\n"},"metadata":{}},{"name":"stdout","text":"\nMachine Learning Pipeline Completed Successfully\n","output_type":"stream"}],"execution_count":11}]}